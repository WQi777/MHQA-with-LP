{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Entity-GCN on WikiHop.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yg-li/QA-KG-RL/blob/master/Entity_GCN_on_WikiHop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4e6IAlpUi05",
        "colab_type": "text"
      },
      "source": [
        "This notebook is about using Entity-GCN, an algorithm using R-GCN on entity-relations graph to solve the multi-hop QA problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQuAXdFMMZ15",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install --no-cache-dir torch-scatter torch-sparse torch-cluster\n",
        "!pip install torch-geometric\n",
        "!pip install https://s3.us-east-2.amazonaws.com/dgl.ai/wheels/cuda10.0/dgl-0.3-cp36-cp36m-manylinux1_x86_64.whl\n",
        "! (if [ \"$(pip freeze | grep spacy | cut -d'=' -f 3)\" != \"2.1.3\" ]; then \\\n",
        "     pip uninstall -y spacy; \\\n",
        "     pip install spacy==2.1.3; \\\n",
        "   fi)\n",
        "!pip install neuralcoref\n",
        "!pip install allennlp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZCwquLR9VM1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import json\n",
        "import itertools\n",
        "import re\n",
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "import spacy\n",
        "from spacy.matcher import PhraseMatcher\n",
        "import neuralcoref\n",
        "from allennlp.commands.elmo import ElmoEmbedder\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "\n",
        "import dgl\n",
        "\n",
        "from torch_geometric.data import Data, Batch\n",
        "from torch_geometric.nn import RGCNConv\n",
        "\n",
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvcKylrJU8z7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "neuralcoref.add_to_pipe(nlp)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "elmo = ElmoEmbedder(cuda_device=0 if torch.cuda.is_available() else -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsXK61rj08Lp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# mount Google Drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "# read in QAngaroo WikiHop\n",
        "wh_data_path='/gdrive/My Drive/Colab Notebooks/CSML/Project/data/qangaroo_v1.1/wikihop'\n",
        "with open(os.path.join(wh_data_path, 'train.json')) as f:\n",
        "  train_src = json.loads(f.read())\n",
        "with open(os.path.join(wh_data_path, 'dev.json')) as f:\n",
        "  dev_src = json.loads(f.read())\n",
        "\n",
        "# # read in HotpotQA\n",
        "# hpqa_data_path = '/gdrive/My Drive/Colab Notebooks/CSML/Project/data/hotpotqa'\n",
        "# with open(os.path.join(hpqa_data_path, 'hotpot_train_v1.1.json')) as f:\n",
        "#   train_src = json.loads(f.read())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9qi6k0T9MFM",
        "colab_type": "text"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xQVn9xr9LMm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class QueryEncoder(nn.Module):\n",
        "  def __init__(self, dropout=0):\n",
        "    super(QueryEncoder, self).__init__()\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    self.lstm1 = nn.LSTM(3072, 256, batch_first=True, bidirectional=True)\n",
        "    self.lstm2 = nn.LSTM(512, 128, batch_first=True, bidirectional=True)\n",
        "    self.hidden_map = nn.Linear(256, 128)\n",
        "    self.cell_map = nn.Linear(256, 128)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    # batch_size is always 1 as encoding happens per query\n",
        "    x, (h_n, c_n) = self.lstm1(x)\n",
        "    x = self.dropout(x)\n",
        "    h_n = self.dropout(F.relu(self.hidden_map(h_n)))\n",
        "    c_n = self.dropout(F.relu(self.cell_map(c_n)))\n",
        "    x, (q, c_n) = self.lstm2(x, (h_n, c_n))\n",
        "    q = self.dropout(q.reshape(1, -1, 256))\n",
        "    return q\n",
        "  \n",
        "class CandidateEncoder(nn.Module):\n",
        "  def __init__(self, dropout=0):\n",
        "    super(CandidateEncoder, self).__init__()\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    self.linear1 = nn.Linear(3072, 256)\n",
        "    # the following FF layers applied to concat of query and cand.\n",
        "    self.linear2 = nn.Linear(512, 1024)\n",
        "    self.linear3 = nn.Linear(1024, 512)\n",
        "    \n",
        "  def forward(self, x, q):\n",
        "    x = self.dropout(F.relu(self.linear1(x)))\n",
        "    x = torch.cat((q, x), dim=-1)\n",
        "    x = self.dropout(F.relu(self.linear2(x)))\n",
        "    x = self.dropout(F.relu(self.linear3(x)))\n",
        "    return x\n",
        "  \n",
        "class PyG_RGCN(nn.Module):\n",
        "  def __init__(self, dropout=0, L=3):\n",
        "    super(RGCN, self).__init__()\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    self.L = L\n",
        "    # all R-GCN layers are sharing weights\n",
        "    self.conv = RGCNConv(512, 512, num_relations=4, num_bases=4)\n",
        "    self.gating = nn.Linear(1024, 1)\n",
        "    \n",
        "  def forward(self, x, edge_index, edge_type):\n",
        "    # L is the number of R-GCN layers\n",
        "    for _ in range(self.L):\n",
        "      print(_, flush=True) # TODO: remove this line\n",
        "      u = self.conv(x, edge_index, edge_type)\n",
        "      a = torch.sigmoid(self.gating(torch.cat((u, x), dim=-1)))\n",
        "      x = self.dropout(torch.tanh(u) * a + x * (1-a))\n",
        "    return x  \n",
        "\n",
        "class OutputLayer(nn.Module):\n",
        "  def __init__(self, dropout=0):\n",
        "    super(OutputLayer, self).__init__()\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    self.linear1 = nn.Linear(768, 256)\n",
        "    self.linear2 = nn.Linear(256, 128)\n",
        "    self.linear3 = nn.Linear(128, 1)\n",
        "    \n",
        "  def forward(self, x, q, node_mask):\n",
        "    # batch_size is 1 as instances have different number of candidates\n",
        "    if not x.shape[0] == q.shape[0]:\n",
        "      x = torch.cat((q.expand(x.shape[0], -1), x), dim=-1)\n",
        "    else:\n",
        "      x = torch.cat((q, x), dim=-1)\n",
        "    x = self.dropout(self.linear1(x))\n",
        "    x = self.dropout(self.linear2(x))\n",
        "    x = self.linear3(x)\n",
        "    x *= node_mask\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "labw0S-477BQ",
        "colab_type": "text"
      },
      "source": [
        "# Build Graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sd3LnbHdAx4x",
        "colab_type": "text"
      },
      "source": [
        "## Extract nodes and edges & Encode mentions with ELMo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UewhVYZsqXsd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_info(query, docs, whole_doc, cands, answer,\n",
        "                 query_encoder, cand_encoder):\n",
        "  ''' extract the information needed to build Entity-GCN's graph\n",
        "  Args:\n",
        "    query: the query\n",
        "    docs: spacy annotated documents\n",
        "    whole_doc: spacy annotated concatenated documents\n",
        "    cands: candidates\n",
        "    answer: the correct answer\n",
        "    query_encoder: the encoder for query\n",
        "    cand_encoder: the encoder for candidates given query embedding\n",
        "  Returns:\n",
        "    nodes: nodes of the graph \\\\ dict{id : candidate id (-1 if query entity)}\n",
        "    node_types: 1 for answer, 0 for other candidates, -1 for query entity\n",
        "    node_embeddings: the contextualized embedding of mentions of candidates\n",
        "    query_embedding: the emebdding of query\n",
        "    doc_based_edges: edges that connect mentions in the same document \n",
        "    match_edges: edges that connect exact match \\\\ set((node1, node2))\n",
        "    coref_edges: edges that connect mentions in the same coreference chain \n",
        "    compl_edges: edges that connect all nodes that have not been connected by \n",
        "                 any other types of edges \\\\ set((node1, node2))\n",
        "  ''' \n",
        "  # extract the query entity\n",
        "  query_entity = ' '.join(query.split(' ')[1:])\n",
        "  cands[query_entity] = -1\n",
        "  \n",
        "  # matcher for candidates and query entity\n",
        "  matcher = PhraseMatcher(nlp.vocab)\n",
        "  patterns = [nlp.make_doc(cand) for cand in cands]\n",
        "  matcher.add(\"CandList\", None, *patterns)\n",
        "\n",
        "  # get embedding of query, q\n",
        "  query_embedding = query_encoder(\n",
        "      torch.as_tensor(\n",
        "          elmo.embed_sentence(\n",
        "              query.split(' ')[0].split('_') + \n",
        "              query.split(' ')[1:]).reshape(1, -1, 3072), \n",
        "          device=device))\n",
        "  # get elmo for all documents\n",
        "  docs_elmo = [torch.as_tensor(d.reshape(1, -1, 3072), device=device)\n",
        "               for d in elmo.embed_sentences(\n",
        "                   [[w.text for w in doc] for doc in docs])]\n",
        "  \n",
        "  nodes = {}\n",
        "  node_types = []\n",
        "  node_embeddings = []\n",
        "  with_edges = set()\n",
        "  \n",
        "  # sets to store edges\n",
        "  doc_based_edges = set()\n",
        "  match_edges = set()\n",
        "  coref_edges = set()\n",
        "  compl_edges = set()\n",
        "  \n",
        "  # auxiliary variables for cross-document coreference\n",
        "  out_coref_clusters = [[m.text for m in c.mentions] \n",
        "                        for c in whole_doc._.coref_clusters]\n",
        "  out_coref_tmps = [set()] * len(out_coref_clusters) # nodes in same coref chain\n",
        "  \n",
        "  # accumulate nodes, add the doc_based & coreference edges\n",
        "  for doc_id, doc in enumerate(docs):\n",
        "    matches = matcher(doc)\n",
        "    # text = ' '.join([toc.text for toc in doc])\n",
        "    in_coref_clusters = [[m.text for m in c.mentions] \n",
        "                         for c in doc._.coref_clusters]\n",
        "    in_coref_tmps = [set()] * len(in_coref_clusters) # nodes in same coref chain\n",
        "    doc_tmp = set() # nodes in the same doc\n",
        "    for _, start, end in matches:\n",
        "      match = doc[start:end].text\n",
        "      new_node = len(nodes)\n",
        "      doc_tmp.add(new_node)\n",
        "      nodes[new_node] = cands.get(match, -1)\n",
        "      node_types.append([1 if match == answer \n",
        "                         else -1 if match == query_entity \n",
        "                         else 0])\n",
        "      match_elmo = docs_elmo[doc_id][:, start:end, :].mean(dim=1, keepdim=True)\n",
        "      node_embeddings.append(cand_encoder(match_elmo, query_embedding))\n",
        "      for i, cluster in enumerate(in_coref_clusters):\n",
        "        if match in cluster:\n",
        "          in_coref_tmps[i].add(new_node)\n",
        "      for i, cluster in enumerate(out_coref_clusters):\n",
        "        if match in cluster:\n",
        "          out_coref_tmps[i].add(new_node)\n",
        "          \n",
        "    for pair in itertools.combinations(doc_tmp, 2):\n",
        "      doc_based_edges.add(pair) # doc_based edges\n",
        "      with_edges.update(pair)\n",
        "    for coref_tmp in in_coref_tmps:\n",
        "      for pair in itertools.combinations(coref_tmp, 2):\n",
        "        coref_edges.add(pair) # within-document coref_edges\n",
        "        with_edges.update(pair)\n",
        "        \n",
        "  # cross-document coref_edges\n",
        "  for coref_tmp in out_coref_tmps:\n",
        "    for pair in itertools.combinations(coref_tmp, 2):\n",
        "      coref_edges.add(pair) # cross-document coref_edges\n",
        "      with_edges.update(pair)\n",
        "      \n",
        "  # add exact match edges\n",
        "  for i, j in itertools.combinations(nodes, 2):\n",
        "    if nodes[i] == nodes[j]:\n",
        "      match_edges.add((i,j))\n",
        "      with_edges.update((i,j))\n",
        "      \n",
        "  # add complement edges\n",
        "  isolated_nodes = set(nodes) - with_edges\n",
        "  if isolated_nodes:\n",
        "    for pair in itertools.combinations(isolated_nodes, 2):\n",
        "      compl_edges.add(pair)\n",
        "      \n",
        "  return nodes, node_types, node_embeddings, query_embedding, \\\n",
        "         [doc_based_edges, match_edges, coref_edges, compl_edges]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VE_E01PXA6ne",
        "colab_type": "text"
      },
      "source": [
        "## PyG graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "na1M8d7XqM2C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_pyg_graph(nodes, node_types, node_embeddings, query_embedding, edges):\n",
        "  # nodes\n",
        "  x = torch.cat(node_embeddings).squeeze()\n",
        "  node_name = torch.tensor(list(nodes.values()), device=device)\n",
        "  tmp = torch.tensor(node_types)\n",
        "  node_mask = (tmp >= 0).to(device, torch.float) # whether the node is in candidate list\n",
        "  y = (tmp > 0).to(device, torch.float) # target \n",
        "\n",
        "  # edges\n",
        "  edge_index = torch.zeros(0, 2).to(device, torch.long)\n",
        "  edge_type = torch.zeros(0).to(device, torch.long)\n",
        "  for i, e in enumerate(edges):\n",
        "    if len(e) > 0:\n",
        "      tmp = torch.tensor(list(e), dtype=torch.float, device=device)\n",
        "      # add edges with swapped direction to make the graph undirected\n",
        "      tmp = torch.cat((tmp, \n",
        "                       torch.index_select(tmp, 1, torch.tensor([1,0], device=device))), \n",
        "                      0)\n",
        "      edge_index = torch.cat((edge_index, tmp.to(torch.long)), 0)\n",
        "      edge_type = torch.cat((edge_type, torch.ones(tmp.shape[0]).to(device, torch.long) * i), 0)\n",
        "\n",
        "  data = Data(x=x, node_name=node_name, node_mask=node_mask, \n",
        "              query=query_embedding.reshape(-1, 256), y=y,\n",
        "              edge_index=edge_index.t().contiguous(), edge_type=edge_type)\n",
        "  return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SijcVUbVDFQ",
        "colab_type": "text"
      },
      "source": [
        "## DGL graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VE-MkrfBVNou",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_dgl_graph(nodes, node_types, \n",
        "                    node_embeddings, query_embedding, edges):\n",
        "  g = dgl.DGLGraph()\n",
        "  \n",
        "  # nodes\n",
        "  g.add_nodes(len(nodes))\n",
        "  g.ndata['x'] = torch.cat(node_embeddings).squeeze()\n",
        "  g.ndata['name'] = torch.tensor(list(nodes.values()), device=device)\n",
        "  tmp = torch.tensor(node_types)\n",
        "  g.ndata['mask'] = (tmp >= 0).to(device, torch.float)\n",
        "  g.ndata['y'] = (tmp > 0).to(device, torch.float)\n",
        "  g.ndata['q'] = query_embedding.reshape(-1, 256).expand(g.number_of_nodes(), -1)\n",
        "  \n",
        "  # edges\n",
        "  edge_index = torch.zeros(0, 2).to(device, torch.long)\n",
        "  edge_type = torch.zeros(0).to(device, torch.long)\n",
        "  for i, e in enumerate(edges):\n",
        "    if len(e) > 0:\n",
        "      tmp = torch.tensor(list(e), dtype=torch.float, device=device)\n",
        "      # add edges with swapped direction to make the graph undirected\n",
        "      tmp = torch.cat((tmp, \n",
        "                       torch.index_select(tmp, 1, torch.tensor([1,0], device=device))), \n",
        "                      0)\n",
        "      edge_index = torch.cat((edge_index, tmp.to(torch.long)), 0)\n",
        "      edge_type = torch.cat((edge_type, torch.ones(tmp.shape[0]).to(device, torch.long) * i), 0)\n",
        "  g.add_edges(edge_index[:,0], edge_index[:,1])    \n",
        "  g.edata['w'] = edge_type\n",
        "  \n",
        "  return g"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoOp15T9Sxb0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_graph(instance, query_encoder, cand_encoder):\n",
        "  query = instance.get('query').strip()\n",
        "  supports = [text.lower() for text in instance.get('supports')]\n",
        "  docs = [nlp(text) for text in supports]\n",
        "  whole_doc = nlp(' '.join(supports))\n",
        "  cands = dict([(v, i) for i, v in \n",
        "                enumerate([cand.lower().strip() \n",
        "                           for cand in instance.get('candidates')])])\n",
        "  answer = instance.get('answer')\n",
        "  \n",
        "  # extract nodes, edges, and embeddings\n",
        "  nodes, node_types, node_embeddings, query_embedding, edges = \\\n",
        "    extract_info(query, docs, whole_doc, cands, answer, \n",
        "                 query_encoder, cand_encoder)\n",
        "  \n",
        "#   # build PyG graph\n",
        "#   data = build_pyg_graph(nodes, node_types, \n",
        "#                          node_embeddings, query_embedding, edges)\n",
        "\n",
        "  # build DGL graph\n",
        "  g = build_dgl_graph(nodes, node_types, \n",
        "                      node_embeddings, query_embedding, edges)\n",
        "  return g # query, cands, answer   TODO: check the output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gG0Ps6ZgicdP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data.node_name.numel()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdURez7sCuPz",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSnfyPOZgIlM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_models(epoch, loss_history, optimizer,\n",
        "                query_encoder, cand_encoder, rgcn, output_layer, PATH):\n",
        "  torch.save({\n",
        "        'epoch': epoch,\n",
        "        'loss_history': loss_history,\n",
        "        'query_encoder': query_encoder.state_dict(),\n",
        "        'cand_encoder': cand_encoder.state_dict(),\n",
        "        'rgcn': rgcn.state_dict(),\n",
        "        'output_layer': output_layer.state_dict(),\n",
        "        'optimizer': optimizer.state_dict()\n",
        "  }, PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2d8cV8Ee01x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "af577646-1e7d-4475-e65a-f68a946e412c"
      },
      "source": [
        ""
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-bda4a9fdd59c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'DGLGraph' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x489ulHRV71k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epochs, batch_size, optimizer, loss_fn, src, query_encoder, \n",
        "          cand_encoder, rgcn, output_layer, PATH, loss_history=[], tol=1e-3):\n",
        "  query_encoder.train()\n",
        "  cand_encoder.train()\n",
        "  rgcn.train()\n",
        "  output_layer.train()\n",
        "  for e in epochs:\n",
        "    random.shuffle(src)\n",
        "    for i in range(len(src)):\n",
        "#       if i % batch_size == 0:\n",
        "        # start of a batch\n",
        "      optimizer.zero_grad()\n",
        "      data = build_graph(src[i], query_encoder, cand_encoder)\n",
        "      out = rgcn(data.x, data.edge_index, data.edge_type)\n",
        "      pred = output_layer(out, data.query, data.node_mask)\n",
        "      loss = loss_fn(pred, data.y) # TODO: maybe need to scale for batch\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      print('Epoch: {:2d}  [{:d}/{:d}]\\tloss: {:.4f}\\t{}'.format(\n",
        "          e, i+1, len(src), loss.item(), datetime.now()), flush=True)\n",
        "      if i != 0 and i % batch_size == 0 or i == len(src)-1:\n",
        "        # end of a batch\n",
        "        \n",
        "        save_models(e, loss_history, optimizer, \n",
        "            query_encoder, cand_encoder, rgcn, output_layer, \n",
        "            PATH)\n",
        "    # end of epoch\n",
        "    save_models(e+1, loss_history, optimizer, \n",
        "        query_encoder, cand_encoder, rgcn, output_layer, \n",
        "        PATH)\n",
        "    if loss_history[-10] - loss_history[-1] < tol:\n",
        "      return query_encoder, cand_encoder, rgcn, output_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pp7lxklJwdt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "outputId": "b807ca95-d02e-4f94-d1cf-da4631979ddd"
      },
      "source": [
        "# parameters\n",
        "epochs = range(20)\n",
        "batch_size = 32\n",
        "L = 1 # number of R-GCN layers\n",
        "lr = 1e-4\n",
        "dropout = 0\n",
        "save_path='/gdrive/My Drive/Colab Notebooks/CSML/Project/checkpoint/entity_gcn_gpu.pt'\n",
        "\n",
        "# models\n",
        "query_encoder = QueryEncoder(dropout=dropout).to(device)\n",
        "cand_encoder = CandidateEncoder(dropout=dropout).to(device)\n",
        "rgcn = PyG_RGCN(dropout=dropout, L=L).to(device)\n",
        "output_layer = OutputLayer(dropout=dropout).to(device)\n",
        "\n",
        "optimizer = Adam(\n",
        "    itertools.chain(\n",
        "        query_encoder.parameters(), \n",
        "        cand_encoder.parameters(), \n",
        "        rgcn.parameters(), \n",
        "        output_layer.parameters()), \n",
        "    lr=lr)\n",
        "loss = nn.BCEWithLogitsLoss()\n",
        "loss_history = []\n",
        "\n",
        "query_encoder, cand_encoder, rgcn, output_layer = train(\n",
        "    epochs, batch_size, optimizer, loss, train_src, query_encoder, \n",
        "    cand_encoder, rgcn, output_layer, save_path, loss_history=loss_history)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-6a756162b3a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m query_encoder, cand_encoder, rgcn, output_layer = train(\n\u001b[1;32m     25\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_src\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_encoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     cand_encoder, rgcn, output_layer, save_path, loss_history=loss_history)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-0d4a9f960008>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, batch_size, optimizer, loss_fn, src, query_encoder, cand_encoder, rgcn, output_layer, PATH, loss_history, tol)\u001b[0m\n\u001b[1;32m     12\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m       \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcand_encoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m       \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrgcn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m       \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# TODO: maybe need to scale for batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-5ca348b66b22>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, edge_type)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# TODO: remove this line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m       \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m       \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgating\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch_geometric/nn/conv/rgcn_conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, edge_type, edge_norm)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;34m\"\"\"\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         return self.propagate(\n\u001b[0;32m---> 69\u001b[0;31m             edge_index, x=x, edge_type=edge_type, edge_norm=edge_norm)\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_j\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index_j\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch_geometric/nn/conv/message_passing.py\u001b[0m in \u001b[0;36mpropagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mupdate_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__update_args__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmessage_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscatter_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mupdate_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch_geometric/nn/conv/rgcn_conv.py\u001b[0m in \u001b[0;36mmessage\u001b[0;34m(self, x_j, edge_index_j, edge_type, edge_norm)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_relations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 37.72 GiB (GPU 0; 11.17 GiB total capacity; 493.13 MiB already allocated; 10.33 GiB free; 32.87 MiB cached)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KcJiKqelPd0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9360a6e3-ff32-4e68-cd7a-649e690ec91e"
      },
      "source": [
        "build_graph(train_src[0], query_encoder, cand_encoder)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data(edge_index=[2, 38630], edge_type=[38630], node_mask=[390, 1], node_name=[390], query=[1, 256], x=[390, 512], y=[390, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTIBcwyIw6qp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}