
@inproceedings{rajpurkar_squad:_2016,
  title = {{SQuAD}: 100,000+ {Questions} for {Machine} {Comprehension} of {Text}},
  shorttitle = {{SQuAD}},
  url = {https://www.aclweb.org/anthology/papers/D/D16/D16-1264/},
  doi = {10.18653/v1/D16-1264},
  language = {en-us},
  urldate = {2019-07-01},
  author = {Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  month = nov,
  year = {2016},
  pages = {2383--2392},
  file = {Full Text PDF:/Users/yl/Zotero/storage/PA5E48HX/Rajpurkar et al. - 2016 - SQuAD 100,000+ Questions for Machine Comprehensio.pdf:application/pdf;Snapshot:/Users/yl/Zotero/storage/BUTWKJ9Q/D16-1264.html:text/html}
}

@incollection{hermann_teaching_2015,
  title = {Teaching {Machines} to {Read} and {Comprehend}},
  url = {http://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend.pdf},
  urldate = {2019-07-01},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 28},
  publisher = {Curran Associates, Inc.},
  author = {Hermann, Karl Moritz and Kocisky, Tomas and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil},
  editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
  year = {2015},
  pages = {1693--1701},
  file = {NIPS Full Text PDF:/Users/yl/Zotero/storage/I5FRJF4Z/Hermann et al. - 2015 - Teaching Machines to Read and Comprehend.pdf:application/pdf;NIPS Snapshot:/Users/yl/Zotero/storage/Q5JRJATV/5945-teaching-machines-to-read-and-comprehend.html:text/html}
}

@article{kocisky_narrativeqa_2018,
  title = {The {NarrativeQA} {Reading} {Comprehension} {Challenge}},
  volume = {6},
  url = {https://www.aclweb.org/anthology/Q18-1023},
  doi = {10.1162/tacl_a_00023},
  abstract = {Reading comprehension (RC)—in contrast to information retrieval—requires integrating information and reasoning about events, entities, and their relations across a full document. Question answering is conventionally used to assess RC ability, in both artificial agents and children learning to read. However, existing RC datasets and tasks are dominated by questions that can be solved by selecting answers using superficial information (e.g., local context similarity or global term frequency); they thus fail to test for the essential integrative aspect of RC. To encourage progress on deeper comprehension of language, we present a new dataset and set of tasks in which the reader must answer questions about stories by reading entire books or movie scripts. These tasks are designed so that successfully answering their questions requires understanding the underlying narrative rather than relying on shallow pattern matching or salience. We show that although humans solve the tasks easily, standard RC models struggle on the tasks presented here. We provide an analysis of the dataset and the challenges it presents.},
  urldate = {2019-07-01},
  journal = {Transactions of the Association for Computational Linguistics},
  author = {Kočiský, Tomáš and Schwarz, Jonathan and Blunsom, Phil and Dyer, Chris and Hermann, Karl Moritz and Melis, Gábor and Grefenstette, Edward},
  year = {2018},
  pages = {317--328},
  file = {Full Text PDF:/Users/yl/Zotero/storage/5IQ5H2MJ/Kočiský et al. - 2018 - The NarrativeQA Reading Comprehension Challenge.pdf:application/pdf}
}

@article{reddy_coqa:_2019,
  title = {{CoQA}: {A} {Conversational} {Question} {Answering} {Challenge}},
  volume = {7},
  copyright = {Copyright (c) 2019 Association for Computational Linguistics},
  issn = {2307-387X},
  shorttitle = {{CoQA}},
  url = {https://transacl.org/ojs/index.php/tacl/article/view/1572},
  abstract = {Humans gather information through conversations involving a series of interconnected questions and answers. For machines to assist in information gathering, it is therefore essential to enable them to answer conversational questions. We introduce CoQA, a novel dataset for building Conversational Question Answering systems. Our dataset contains 127k questions with answers, obtained from 8k conversations about text passages from seven diverse domains. The questions are conversational, and the answers are free-form text with their corresponding evidence highlighted in the passage. We analyze CoQA in depth and show that conversational questions have challenging phenomena not present in existing reading comprehension datasets, e.g., coreference and pragmatic reasoning. We evaluate strong dialogue and reading comprehension models on CoQA. The best system obtains an F1 score of 65.4\%, which is 23.4 points behind human performance (88.8\%), indicating there is ample room for improvement. We present CoQA as a challenge to the community at https://stanfordnlp.github.io/coqa},
  language = {en},
  number = {0},
  urldate = {2019-07-01},
  journal = {Transactions of the Association for Computational Linguistics},
  author = {Reddy, Siva and Chen, Danqi and Manning, Christopher D.},
  month = may,
  year = {2019},
  pages = {249--266},
  file = {Snapshot:/Users/yl/Zotero/storage/HCQ75H8U/1572.html:text/html}
}

@inproceedings{joshi_triviaqa:_2017,
  title = {{TriviaQA}: {A} {Large} {Scale} {Distantly} {Supervised} {Challenge} {Dataset} for {Reading} {Comprehension}},
  shorttitle = {{TriviaQA}},
  url = {https://www.aclweb.org/anthology/papers/P/P17/P17-1147/},
  doi = {10.18653/v1/P17-1147},
  language = {en-us},
  urldate = {2019-07-01},
  author = {Joshi, Mandar and Choi, Eunsol and Weld, Daniel S. and Zettlemoyer, Luke},
  month = jul,
  year = {2017},
  pages = {1601--1611},
  file = {Full Text PDF:/Users/yl/Zotero/storage/S7BZZ9PP/Joshi et al. - 2017 - TriviaQA A Large Scale Distantly Supervised Chall.pdf:application/pdf;Snapshot:/Users/yl/Zotero/storage/BW6IX7I2/P17-1147.html:text/html}
}

@inproceedings{lai_race:_2017,
  address = {Copenhagen, Denmark},
  title = {{RACE}: {Large}-scale {ReAding} {Comprehension} {Dataset} {From} {Examinations}},
  shorttitle = {{RACE}},
  url = {https://www.aclweb.org/anthology/D17-1082},
  doi = {10.18653/v1/D17-1082},
  abstract = {We present RACE, a new dataset for benchmark evaluation of methods in the reading comprehension task. Collected from the English exams for middle and high school Chinese students in the age range between 12 to 18, RACE consists of near 28,000 passages and near 100,000 questions generated by human experts (English instructors), and covers a variety of topics which are carefully designed for evaluating the students' ability in understanding and reasoning. In particular, the proportion of questions that requires reasoning is much larger in RACE than that in other benchmark datasets for reading comprehension, and there is a significant gap between the performance of the state-of-the-art models (43\%) and the ceiling human performance (95\%). We hope this new dataset can serve as a valuable resource for research and evaluation in machine comprehension. The dataset is freely available at http://www.cs.cmu.edu/ glai1/data/race/ and the code is available at https://github.com/qizhex/RACE\_AR\_baselines.},
  urldate = {2019-07-01},
  booktitle = {Proceedings of the 2017 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
  publisher = {Association for Computational Linguistics},
  author = {Lai, Guokun and Xie, Qizhe and Liu, Hanxiao and Yang, Yiming and Hovy, Eduard},
  month = sep,
  year = {2017},
  pages = {785--794},
  file = {Full Text PDF:/Users/yl/Zotero/storage/65T9DVMY/Lai et al. - 2017 - RACE Large-scale ReAding Comprehension Dataset Fr.pdf:application/pdf}
}

@article{seo_bidirectional_2016,
  title = {Bidirectional {Attention} {Flow} for {Machine} {Comprehension}},
  url = {http://arxiv.org/abs/1611.01603},
  abstract = {Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.},
  urldate = {2019-07-01},
  journal = {arXiv:1611.01603 [cs]},
  author = {Seo, Minjoon and Kembhavi, Aniruddha and Farhadi, Ali and Hajishirzi, Hannaneh},
  month = nov,
  year = {2016},
  note = {arXiv: 1611.01603},
  keywords = {Computer Science - Computation and Language},
  file = {arXiv\:1611.01603 PDF:/Users/yl/Zotero/storage/QDB3UAHP/Seo et al. - 2016 - Bidirectional Attention Flow for Machine Comprehen.pdf:application/pdf;arXiv.org Snapshot:/Users/yl/Zotero/storage/X37PIWD7/1611.html:text/html}
}

@article{xiong_dynamic_2016,
  title = {Dynamic {Coattention} {Networks} {For} {Question} {Answering}},
  url = {http://arxiv.org/abs/1611.01604},
  abstract = {Several deep learning models have been proposed for question answering. However, due to their single-pass nature, they have no way to recover from local maxima corresponding to incorrect answers. To address this problem, we introduce the Dynamic Coattention Network (DCN) for question answering. The DCN first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both. Then a dynamic pointing decoder iterates over potential answer spans. This iterative procedure enables the model to recover from initial local maxima corresponding to incorrect answers. On the Stanford question answering dataset, a single DCN model improves the previous state of the art from 71.0\% F1 to 75.9\%, while a DCN ensemble obtains 80.4\% F1.},
  urldate = {2019-07-01},
  journal = {arXiv:1611.01604 [cs]},
  author = {Xiong, Caiming and Zhong, Victor and Socher, Richard},
  month = nov,
  year = {2016},
  note = {arXiv: 1611.01604},
  keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
  file = {arXiv\:1611.01604 PDF:/Users/yl/Zotero/storage/N97GTFS6/Xiong et al. - 2016 - Dynamic Coattention Networks For Question Answerin.pdf:application/pdf;arXiv.org Snapshot:/Users/yl/Zotero/storage/U6TQ3WL7/1611.html:text/html}
}

@inproceedings{shen_reasonet:_2017,
  address = {New York, NY, USA},
  series = {{KDD} '17},
  title = {{ReasoNet}: {Learning} to {Stop} {Reading} in {Machine} {Comprehension}},
  isbn = {978-1-4503-4887-4},
  shorttitle = {{ReasoNet}},
  url = {http://doi.acm.org/10.1145/3097983.3098177},
  doi = {10.1145/3097983.3098177},
  abstract = {Teaching a computer to read and answer general questions pertaining to a document is a challenging yet unsolved problem. In this paper, we describe a novel neural network architecture called the Reasoning Network (ReasoNet) for machine comprehension tasks. ReasoNets make use of multiple turns to effectively exploit and then reason over the relation among queries, documents, and answers. Different from previous approaches using a fixed number of turns during inference, ReasoNets introduce a termination state to relax this constraint on the reasoning depth. With the use of reinforcement learning, ReasoNets can dynamically determine whether to continue the comprehension process after digesting intermediate results, or to terminate reading when it concludes that existing information is adequate to produce an answer. ReasoNets achieve superior performance in machine comprehension datasets, including unstructured CNN and Daily Mail datasets, the Stanford SQuAD dataset, and a structured Graph Reachability dataset.},
  urldate = {2019-07-01},
  booktitle = {Proceedings of the 23rd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
  publisher = {ACM},
  author = {Shen, Yelong and Huang, Po-Sen and Gao, Jianfeng and Chen, Weizhu},
  year = {2017},
  note = {event-place: Halifax, NS, Canada},
  keywords = {deep reinforcement learning, machine reading comprehension, reasonet},
  pages = {1047--1055},
  file = {ACM Full Text PDF:/Users/yl/Zotero/storage/CEH7LLZI/Shen et al. - 2017 - ReasoNet Learning to Stop Reading in Machine Comp.pdf:application/pdf}
}

@inproceedings{weissenborn_making_2017,
  address = {Vancouver, Canada},
  title = {Making {Neural} {QA} as {Simple} as {Possible} but not {Simpler}},
  url = {https://www.aclweb.org/anthology/K17-1028},
  doi = {10.18653/v1/K17-1028},
  abstract = {Recent development of large-scale question answering (QA) datasets triggered a substantial amount of research into end-to-end neural architectures for QA. Increasingly complex systems have been conceived without comparison to simpler neural baseline systems that would justify their complexity. In this work, we propose a simple heuristic that guides the development of neural baseline systems for the extractive QA task. We find that there are two ingredients necessary for building a high-performing neural QA system: first, the awareness of question words while processing the context and second, a composition function that goes beyond simple bag-of-words modeling, such as recurrent neural networks. Our results show that FastQA, a system that meets these two requirements, can achieve very competitive performance compared with existing models. We argue that this surprising finding puts results of previous systems and the complexity of recent QA datasets into perspective.},
  urldate = {2019-07-01},
  booktitle = {Proceedings of the 21st {Conference} on {Computational} {Natural} {Language} {Learning} ({CoNLL} 2017)},
  publisher = {Association for Computational Linguistics},
  author = {Weissenborn, Dirk and Wiese, Georg and Seiffe, Laura},
  month = aug,
  year = {2017},
  pages = {271--280},
  file = {Full Text PDF:/Users/yl/Zotero/storage/A8KRYFRW/Weissenborn et al. - 2017 - Making Neural QA as Simple as Possible but not Sim.pdf:application/pdf}
}


@article{welbl_constructing_2018,
  title = {Constructing {Datasets} for {Multi}-hop {Reading} {Comprehension} {Across} {Documents}},
  volume = {6},
  url = {https://www.aclweb.org/anthology/Q18-1021},
  doi = {10.1162/tacl_a_00021},
  abstract = {Most Reading Comprehension methods limit themselves to queries which can be answered using a single sentence, paragraph, or document. Enabling models to combine disjoint pieces of textual evidence would extend the scope of machine comprehension methods, but currently no resources exist to train and test this capability. We propose a novel task to encourage the development of models for text understanding across multiple documents and to investigate the limits of existing methods. In our task, a model learns to seek and combine evidence — effectively performing multihop, alias multi-step, inference. We devise a methodology to produce datasets for this task, given a collection of query-answer pairs and thematically linked documents. Two datasets from different domains are induced, and we identify potential pitfalls and devise circumvention strategies. We evaluate two previously proposed competitive models and find that one can integrate information across documents. However, both models struggle to select relevant information; and providing documents guaranteed to be relevant greatly improves their performance. While the models outperform several strong baselines, their best accuracy reaches 54.5\% on an annotated test set, compared to human performance at 85.0\%, leaving ample room for improvement.},
  urldate = {2019-07-01},
  journal = {Transactions of the Association for Computational Linguistics},
  author = {Welbl, Johannes and Stenetorp, Pontus and Riedel, Sebastian},
  year = {2018},
  pages = {287--302},
  file = {Full Text PDF:/Users/yl/Zotero/storage/X9VTEBRA/Welbl et al. - 2018 - Constructing Datasets for Multi-hop Reading Compre.pdf:application/pdf}
}

@inproceedings{yang_hotpotqa:_2018,
  address = {Brussels, Belgium},
  title = {{HotpotQA}: {A} {Dataset} for {Diverse}, {Explainable} {Multi}-hop {Question} {Answering}},
  shorttitle = {{HotpotQA}},
  url = {https://www.aclweb.org/anthology/D18-1259},
  abstract = {Existing question answering (QA) datasets fail to train QA systems to perform complex reasoning and provide explanations for answers. We introduce HotpotQA, a new dataset with 113k Wikipedia-based question-answer pairs with four key features: (1) the questions require finding and reasoning over multiple supporting documents to answer; (2) the questions are diverse and not constrained to any pre-existing knowledge bases or knowledge schemas; (3) we provide sentence-level supporting facts required for reasoning, allowing QA systems to reason with strong supervision and explain the predictions; (4) we offer a new type of factoid comparison questions to test QA systems' ability to extract relevant facts and perform necessary comparison. We show that HotpotQA is challenging for the latest QA systems, and the supporting facts enable models to improve performance and make explainable predictions.},
  urldate = {2019-07-01},
  booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
  publisher = {Association for Computational Linguistics},
  author = {Yang, Zhilin and Qi, Peng and Zhang, Saizheng and Bengio, Yoshua and Cohen, William and Salakhutdinov, Ruslan and Manning, Christopher D.},
  month = oct,
  year = {2018},
  pages = {2369--2380},
  file = {Full Text PDF:/Users/yl/Zotero/storage/HU385PHV/Yang et al. - 2018 - HotpotQA A Dataset for Diverse, Explainable Multi.pdf:application/pdf}
}
