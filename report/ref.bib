
@inproceedings{rajpurkar_squad:_2016,
	title = {{SQuAD}: 100,000+ {Questions} for {Machine} {Comprehension} of {Text}},
	shorttitle = {{SQuAD}},
	url = {https://www.aclweb.org/anthology/papers/D/D16/D16-1264/},
	doi = {10.18653/v1/D16-1264},
	language = {en-us},
	urldate = {2019-07-01},
	booktitle = {Proceedings of the 2016 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	author = {Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
	month = nov,
	year = {2016},
	pages = {2383--2392},
	file = {Full Text PDF:/Users/yl/Zotero/storage/PA5E48HX/Rajpurkar et al. - 2016 - SQuAD 100,000+ Questions for Machine Comprehensio.pdf:application/pdf;Snapshot:/Users/yl/Zotero/storage/BUTWKJ9Q/D16-1264.html:text/html}
}

@incollection{hermann_teaching_2015,
	title = {Teaching {Machines} to {Read} and {Comprehend}},
	url = {http://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend.pdf},
	urldate = {2019-07-01},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 28},
	publisher = {Curran Associates, Inc.},
	author = {Hermann, Karl Moritz and Kocisky, Tomas and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil},
	editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
	year = {2015},
	pages = {1693--1701},
	file = {NIPS Full Text PDF:/Users/yl/Zotero/storage/I5FRJF4Z/Hermann et al. - 2015 - Teaching Machines to Read and Comprehend.pdf:application/pdf;NIPS Snapshot:/Users/yl/Zotero/storage/Q5JRJATV/5945-teaching-machines-to-read-and-comprehend.html:text/html}
}

@article{kocisky_narrativeqa_2018,
	title = {The {NarrativeQA} {Reading} {Comprehension} {Challenge}},
	volume = {6},
	url = {https://www.aclweb.org/anthology/Q18-1023},
	doi = {10.1162/tacl_a_00023},
	abstract = {Reading comprehension (RC)—in contrast to information retrieval—requires integrating information and reasoning about events, entities, and their relations across a full document. Question answering is conventionally used to assess RC ability, in both artificial agents and children learning to read. However, existing RC datasets and tasks are dominated by questions that can be solved by selecting answers using superficial information (e.g., local context similarity or global term frequency); they thus fail to test for the essential integrative aspect of RC. To encourage progress on deeper comprehension of language, we present a new dataset and set of tasks in which the reader must answer questions about stories by reading entire books or movie scripts. These tasks are designed so that successfully answering their questions requires understanding the underlying narrative rather than relying on shallow pattern matching or salience. We show that although humans solve the tasks easily, standard RC models struggle on the tasks presented here. We provide an analysis of the dataset and the challenges it presents.},
	urldate = {2019-07-01},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Kočiský, Tomáš and Schwarz, Jonathan and Blunsom, Phil and Dyer, Chris and Hermann, Karl Moritz and Melis, Gábor and Grefenstette, Edward},
	year = {2018},
	pages = {317--328},
	file = {Full Text PDF:/Users/yl/Zotero/storage/5IQ5H2MJ/Kočiský et al. - 2018 - The NarrativeQA Reading Comprehension Challenge.pdf:application/pdf}
}

@article{reddy_coqa:_2019,
	title = {{CoQA}: {A} {Conversational} {Question} {Answering} {Challenge}},
	volume = {7},
	copyright = {Copyright (c) 2019 Association for Computational Linguistics},
	issn = {2307-387X},
	shorttitle = {{CoQA}},
	url = {https://transacl.org/ojs/index.php/tacl/article/view/1572},
	abstract = {Humans gather information through conversations involving a series of interconnected questions and answers. For machines to assist in information gathering, it is therefore essential to enable them to answer conversational questions. We introduce CoQA, a novel dataset for building Conversational Question Answering systems. Our dataset contains 127k questions with answers, obtained from 8k conversations about text passages from seven diverse domains. The questions are conversational, and the answers are free-form text with their corresponding evidence highlighted in the passage. We analyze CoQA in depth and show that conversational questions have challenging phenomena not present in existing reading comprehension datasets, e.g., coreference and pragmatic reasoning. We evaluate strong dialogue and reading comprehension models on CoQA. The best system obtains an F1 score of 65.4\%, which is 23.4 points behind human performance (88.8\%), indicating there is ample room for improvement. We present CoQA as a challenge to the community at https://stanfordnlp.github.io/coqa},
	language = {en},
	number = {0},
	urldate = {2019-07-01},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Reddy, Siva and Chen, Danqi and Manning, Christopher D.},
	month = may,
	year = {2019},
	pages = {249--266},
	file = {Snapshot:/Users/yl/Zotero/storage/HCQ75H8U/1572.html:text/html}
}

@inproceedings{joshi_triviaqa:_2017,
	title = {{TriviaQA}: {A} {Large} {Scale} {Distantly} {Supervised} {Challenge} {Dataset} for {Reading} {Comprehension}},
	shorttitle = {{TriviaQA}},
	url = {https://www.aclweb.org/anthology/papers/P/P17/P17-1147/},
	doi = {10.18653/v1/P17-1147},
	language = {en-us},
	urldate = {2019-07-01},
	booktitle = {Proceedings of the 55th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	author = {Joshi, Mandar and Choi, Eunsol and Weld, Daniel S. and Zettlemoyer, Luke},
	month = jul,
	year = {2017},
	pages = {1601--1611},
	file = {Full Text PDF:/Users/yl/Zotero/storage/S7BZZ9PP/Joshi et al. - 2017 - TriviaQA A Large Scale Distantly Supervised Chall.pdf:application/pdf;Snapshot:/Users/yl/Zotero/storage/BW6IX7I2/P17-1147.html:text/html}
}

@inproceedings{lai_race:_2017,
	address = {Copenhagen, Denmark},
	title = {{RACE}: {Large}-scale {ReAding} {Comprehension} {Dataset} {From} {Examinations}},
	shorttitle = {{RACE}},
	url = {https://www.aclweb.org/anthology/D17-1082},
	doi = {10.18653/v1/D17-1082},
	abstract = {We present RACE, a new dataset for benchmark evaluation of methods in the reading comprehension task. Collected from the English exams for middle and high school Chinese students in the age range between 12 to 18, RACE consists of near 28,000 passages and near 100,000 questions generated by human experts (English instructors), and covers a variety of topics which are carefully designed for evaluating the students' ability in understanding and reasoning. In particular, the proportion of questions that requires reasoning is much larger in RACE than that in other benchmark datasets for reading comprehension, and there is a significant gap between the performance of the state-of-the-art models (43\%) and the ceiling human performance (95\%). We hope this new dataset can serve as a valuable resource for research and evaluation in machine comprehension. The dataset is freely available at http://www.cs.cmu.edu/ glai1/data/race/ and the code is available at https://github.com/qizhex/RACE\_AR\_baselines.},
	urldate = {2019-07-01},
	booktitle = {Proceedings of the 2017 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Lai, Guokun and Xie, Qizhe and Liu, Hanxiao and Yang, Yiming and Hovy, Eduard},
	month = sep,
	year = {2017},
	pages = {785--794},
	file = {Full Text PDF:/Users/yl/Zotero/storage/65T9DVMY/Lai et al. - 2017 - RACE Large-scale ReAding Comprehension Dataset Fr.pdf:application/pdf}
}

@article{xiong_dynamic_2016,
	title = {Dynamic {Coattention} {Networks} {For} {Question} {Answering}},
	url = {http://arxiv.org/abs/1611.01604},
	abstract = {Several deep learning models have been proposed for question answering. However, due to their single-pass nature, they have no way to recover from local maxima corresponding to incorrect answers. To address this problem, we introduce the Dynamic Coattention Network (DCN) for question answering. The DCN first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both. Then a dynamic pointing decoder iterates over potential answer spans. This iterative procedure enables the model to recover from initial local maxima corresponding to incorrect answers. On the Stanford question answering dataset, a single DCN model improves the previous state of the art from 71.0\% F1 to 75.9\%, while a DCN ensemble obtains 80.4\% F1.},
	urldate = {2019-07-01},
	journal = {arXiv:1611.01604 [cs]},
	author = {Xiong, Caiming and Zhong, Victor and Socher, Richard},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.01604},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv\:1611.01604 PDF:/Users/yl/Zotero/storage/N97GTFS6/Xiong et al. - 2016 - Dynamic Coattention Networks For Question Answerin.pdf:application/pdf;arXiv.org Snapshot:/Users/yl/Zotero/storage/U6TQ3WL7/1611.html:text/html}
}

@inproceedings{shen_reasonet:_2017,
	address = {New York, NY, USA},
	series = {{KDD} '17},
	title = {{ReasoNet}: {Learning} to {Stop} {Reading} in {Machine} {Comprehension}},
	isbn = {978-1-4503-4887-4},
	shorttitle = {{ReasoNet}},
	url = {http://doi.acm.org/10.1145/3097983.3098177},
	doi = {10.1145/3097983.3098177},
	abstract = {Teaching a computer to read and answer general questions pertaining to a document is a challenging yet unsolved problem. In this paper, we describe a novel neural network architecture called the Reasoning Network (ReasoNet) for machine comprehension tasks. ReasoNets make use of multiple turns to effectively exploit and then reason over the relation among queries, documents, and answers. Different from previous approaches using a fixed number of turns during inference, ReasoNets introduce a termination state to relax this constraint on the reasoning depth. With the use of reinforcement learning, ReasoNets can dynamically determine whether to continue the comprehension process after digesting intermediate results, or to terminate reading when it concludes that existing information is adequate to produce an answer. ReasoNets achieve superior performance in machine comprehension datasets, including unstructured CNN and Daily Mail datasets, the Stanford SQuAD dataset, and a structured Graph Reachability dataset.},
	urldate = {2019-07-01},
	booktitle = {Proceedings of the 23rd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {ACM},
	author = {Shen, Yelong and Huang, Po-Sen and Gao, Jianfeng and Chen, Weizhu},
	year = {2017},
	note = {event-place: Halifax, NS, Canada},
	keywords = {deep reinforcement learning, machine reading comprehension, reasonet},
	pages = {1047--1055},
	file = {ACM Full Text PDF:/Users/yl/Zotero/storage/CEH7LLZI/Shen et al. - 2017 - ReasoNet Learning to Stop Reading in Machine Comp.pdf:application/pdf}
}

@inproceedings{weissenborn_making_2017,
	address = {Vancouver, Canada},
	title = {Making {Neural} {QA} as {Simple} as {Possible} but not {Simpler}},
	url = {https://www.aclweb.org/anthology/K17-1028},
	doi = {10.18653/v1/K17-1028},
	abstract = {Recent development of large-scale question answering (QA) datasets triggered a substantial amount of research into end-to-end neural architectures for QA. Increasingly complex systems have been conceived without comparison to simpler neural baseline systems that would justify their complexity. In this work, we propose a simple heuristic that guides the development of neural baseline systems for the extractive QA task. We find that there are two ingredients necessary for building a high-performing neural QA system: first, the awareness of question words while processing the context and second, a composition function that goes beyond simple bag-of-words modeling, such as recurrent neural networks. Our results show that FastQA, a system that meets these two requirements, can achieve very competitive performance compared with existing models. We argue that this surprising finding puts results of previous systems and the complexity of recent QA datasets into perspective.},
	urldate = {2019-07-01},
	booktitle = {Proceedings of the 21st {Conference} on {Computational} {Natural} {Language} {Learning} ({CoNLL} 2017)},
	publisher = {Association for Computational Linguistics},
	author = {Weissenborn, Dirk and Wiese, Georg and Seiffe, Laura},
	month = aug,
	year = {2017},
	pages = {271--280},
	file = {Full Text PDF:/Users/yl/Zotero/storage/A8KRYFRW/Weissenborn et al. - 2017 - Making Neural QA as Simple as Possible but not Sim.pdf:application/pdf}
}

@article{welbl_constructing_2018,
	title = {Constructing {Datasets} for {Multi}-hop {Reading} {Comprehension} {Across} {Documents}},
	volume = {6},
	url = {https://www.aclweb.org/anthology/Q18-1021},
	doi = {10.1162/tacl_a_00021},
	abstract = {Most Reading Comprehension methods limit themselves to queries which can be answered using a single sentence, paragraph, or document. Enabling models to combine disjoint pieces of textual evidence would extend the scope of machine comprehension methods, but currently no resources exist to train and test this capability. We propose a novel task to encourage the development of models for text understanding across multiple documents and to investigate the limits of existing methods. In our task, a model learns to seek and combine evidence — effectively performing multihop, alias multi-step, inference. We devise a methodology to produce datasets for this task, given a collection of query-answer pairs and thematically linked documents. Two datasets from different domains are induced, and we identify potential pitfalls and devise circumvention strategies. We evaluate two previously proposed competitive models and find that one can integrate information across documents. However, both models struggle to select relevant information; and providing documents guaranteed to be relevant greatly improves their performance. While the models outperform several strong baselines, their best accuracy reaches 54.5\% on an annotated test set, compared to human performance at 85.0\%, leaving ample room for improvement.},
	urldate = {2019-07-01},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Welbl, Johannes and Stenetorp, Pontus and Riedel, Sebastian},
	year = {2018},
	pages = {287--302},
	file = {Full Text PDF:/Users/yl/Zotero/storage/X9VTEBRA/Welbl et al. - 2018 - Constructing Datasets for Multi-hop Reading Compre.pdf:application/pdf}
}

@inproceedings{schlichtkrull_modeling_2018,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Modeling {Relational} {Data} with {Graph} {Convolutional} {Networks}},
	isbn = {978-3-319-93417-4},
	abstract = {Knowledge graphs enable a wide variety of applications, including question answering and information retrieval. Despite the great effort invested in their creation and maintenance, even the largest (e.g., Yago, DBPedia or Wikidata) remain incomplete. We introduce Relational Graph Convolutional Networks (R-GCNs) and apply them to two standard knowledge base completion tasks: Link prediction (recovery of missing facts, i.e. subject-predicate-object triples) and entity classification (recovery of missing entity attributes). R-GCNs are related to a recent class of neural networks operating on graphs, and are developed specifically to handle the highly multi-relational data characteristic of realistic knowledge bases. We demonstrate the effectiveness of R-GCNs as a stand-alone model for entity classification. We further show that factorization models for link prediction such as DistMult can be significantly improved through the use of an R-GCN encoder model to accumulate evidence over multiple inference steps in the graph, demonstrating a large improvement of 29.8\% on FB15k-237 over a decoder-only baseline.},
	language = {en},
	booktitle = {The {Semantic} {Web}},
	publisher = {Springer International Publishing},
	author = {Schlichtkrull, Michael and Kipf, Thomas N. and Bloem, Peter and van den Berg, Rianne and Titov, Ivan and Welling, Max},
	editor = {Gangemi, Aldo and Navigli, Roberto and Vidal, Maria-Esther and Hitzler, Pascal and Troncy, Raphaël and Hollink, Laura and Tordai, Anna and Alam, Mehwish},
	year = {2018},
	pages = {593--607},
	file = {Springer Full Text PDF:/Users/yl/Zotero/storage/TYJT4F53/Schlichtkrull et al. - 2018 - Modeling Relational Data with Graph Convolutional .pdf:application/pdf}
}

@inproceedings{vrandecic_wikidata:_2012,
	title = {Wikidata: a new platform for collaborative data collection},
	shorttitle = {Wikidata},
	doi = {10.1145/2187980.2188242},
	abstract = {This year, Wikimedia starts to build a new platform for the collaborative acquisition and maintenance of structured data: Wikidata. Wikidata's prime purpose is to be used within the other Wikimedia projects, like Wikipedia, to provide well-maintained, high-quality data. The nature and requirements of the Wikimedia projects require to develop a few novel, or at least unusual features for Wikidata: Wikidata will be a secondary database, i.e. instead of containing facts it will contain references for facts. It will be fully internationalized. It will contain inconsistent and contradictory facts, in order to represent the diversity of knowledge about a given entity.},
	booktitle = {{WWW}},
	author = {Vrandecic, Denny},
	year = {2012},
	keywords = {Requirement, Wikidata, Wikipedia}
}

@inproceedings{peters_deep_2018,
	address = {New Orleans, Louisiana},
	title = {Deep {Contextualized} {Word} {Representations}},
	url = {https://www.aclweb.org/anthology/N18-1202},
	doi = {10.18653/v1/N18-1202},
	abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
	urldate = {2019-07-04},
	booktitle = {Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Peters, Matthew and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
	month = jun,
	year = {2018},
	pages = {2227--2237},
	file = {Full Text PDF:/Users/yl/Zotero/storage/SK6CE3AZ/Peters et al. - 2018 - Deep Contextualized Word Representations.pdf:application/pdf}
}

@article{kipf_semi-supervised_2016,
	title = {Semi-{Supervised} {Classification} with {Graph} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1609.02907},
	abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
	urldate = {2019-07-04},
	journal = {arXiv:1609.02907 [cs, stat]},
	author = {Kipf, Thomas N. and Welling, Max},
	month = sep,
	year = {2016},
	note = {arXiv: 1609.02907},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1609.02907 PDF:/Users/yl/Zotero/storage/5R9J6KGI/Kipf and Welling - 2016 - Semi-Supervised Classification with Graph Convolut.pdf:application/pdf;arXiv.org Snapshot:/Users/yl/Zotero/storage/3T29FZZM/1609.html:text/html}
}

@article{micikevicius_mixed_2017,
	title = {Mixed {Precision} {Training}},
	url = {http://arxiv.org/abs/1710.03740},
	abstract = {Deep neural networks have enabled progress in a wide variety of applications. Growing the size of the neural network typically results in improved accuracy. As model sizes grow, the memory and compute requirements for training these models also increases. We introduce a technique to train deep neural networks using half precision floating point numbers. In our technique, weights, activations and gradients are stored in IEEE half-precision format. Half-precision floating numbers have limited numerical range compared to single-precision numbers. We propose two techniques to handle this loss of information. Firstly, we recommend maintaining a single-precision copy of the weights that accumulates the gradients after each optimizer step. This single-precision copy is rounded to half-precision format during training. Secondly, we propose scaling the loss appropriately to handle the loss of information with half-precision gradients. We demonstrate that this approach works for a wide variety of models including convolution neural networks, recurrent neural networks and generative adversarial networks. This technique works for large scale models with more than 100 million parameters trained on large datasets. Using this approach, we can reduce the memory consumption of deep learning models by nearly 2x. In future processors, we can also expect a significant computation speedup using half-precision hardware units.},
	urldate = {2019-07-31},
	journal = {arXiv:1710.03740 [cs, stat]},
	author = {Micikevicius, Paulius and Narang, Sharan and Alben, Jonah and Diamos, Gregory and Elsen, Erich and Garcia, David and Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii and Venkatesh, Ganesh and Wu, Hao},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.03740},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	file = {arXiv\:1710.03740 PDF:/Users/yl/Zotero/storage/GK8FUTAD/Micikevicius et al. - 2017 - Mixed Precision Training.pdf:application/pdf;arXiv.org Snapshot:/Users/yl/Zotero/storage/RS6YN2EI/1710.html:text/html}
}

@inproceedings{de_cao_question_2019,
	address = {Minneapolis, Minnesota},
	title = {Question {Answering} by {Reasoning} {Across} {Documents} with {Graph} {Convolutional} {Networks}},
	url = {https://www.aclweb.org/anthology/N19-1240},
	doi = {10.18653/v1/N19-1240},
	abstract = {Most research in reading comprehension has focused on answering questions based on individual documents or even single paragraphs. We introduce a neural model which integrates and reasons relying on information spread within documents and across multiple documents. We frame it as an inference problem on a graph. Mentions of entities are nodes of this graph while edges encode relations between different mentions (e.g., within- and cross-document co-reference). Graph convolutional networks (GCNs) are applied to these graphs and trained to perform multi-step reasoning. Our Entity-GCN method is scalable and compact, and it achieves state-of-the-art results on a multi-document question answering dataset, WikiHop (Welbl et al., 2018).},
	urldate = {2019-08-05},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {De Cao, Nicola and Aziz, Wilker and Titov, Ivan},
	month = jun,
	year = {2019},
	pages = {2306--2317},
	file = {Full Text PDF:/Users/yl/Zotero/storage/8EV3V6AU/De Cao et al. - 2019 - Question Answering by Reasoning Across Documents w.pdf:application/pdf}
}

@article{song_exploring_2018,
	title = {Exploring {Graph}-structured {Passage} {Representation} for {Multi}-hop {Reading} {Comprehension} with {Graph} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1809.02040},
	abstract = {Multi-hop reading comprehension focuses on one type of factoid question, where a system needs to properly integrate multiple pieces of evidence to correctly answer a question. Previous work approximates global evidence with local coreference information, encoding coreference chains with DAG-styled GRU layers within a gated-attention reader. However, coreference is limited in providing information for rich inference. We introduce a new method for better connecting global evidence, which forms more complex graphs compared to DAGs. To perform evidence integration on our graphs, we investigate two recent graph neural networks, namely graph convolutional network (GCN) and graph recurrent network (GRN). Experiments on two standard datasets show that richer global information leads to better answers. Our method performs better than all published results on these datasets.},
	urldate = {2019-08-05},
	journal = {arXiv:1809.02040 [cs]},
	author = {Song, Linfeng and Wang, Zhiguo and Yu, Mo and Zhang, Yue and Florian, Radu and Gildea, Daniel},
	month = sep,
	year = {2018},
	note = {arXiv: 1809.02040},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv\:1809.02040 PDF:/Users/yl/Zotero/storage/JNJNS4NV/Song et al. - 2018 - Exploring Graph-structured Passage Representation .pdf:application/pdf;arXiv.org Snapshot:/Users/yl/Zotero/storage/XA77CHME/1809.html:text/html}
}

@article{zhou_graph_2018,
	title = {Graph {Neural} {Networks}: {A} {Review} of {Methods} and {Applications}},
	shorttitle = {Graph {Neural} {Networks}},
	url = {http://arxiv.org/abs/1812.08434},
	abstract = {Lots of learning tasks require dealing with graph data which contains rich relation information among elements. Modeling physics system, learning molecular fingerprints, predicting protein interface, and classifying diseases require a model to learn from graph inputs. In other domains such as learning from non-structural data like texts and images, reasoning on extracted structures, like the dependency tree of sentences and the scene graph of images, is an important research topic which also needs graph reasoning models. Graph neural networks (GNNs) are connectionist models that capture the dependence of graphs via message passing between the nodes of graphs. Unlike standard neural networks, graph neural networks retain a state that can represent information from its neighborhood with arbitrary depth. Although the primitive GNNs have been found difficult to train for a fixed point, recent advances in network architectures, optimization techniques, and parallel computation have enabled successful learning with them. In recent years, systems based on variants of graph neural networks such as graph convolutional network (GCN), graph attention network (GAT), gated graph neural network (GGNN) have demonstrated ground-breaking performance on many tasks mentioned above. In this survey, we provide a detailed review over existing graph neural network models, systematically categorize the applications, and propose four open problems for future research.},
	urldate = {2019-08-05},
	journal = {arXiv:1812.08434 [cs, stat]},
	author = {Zhou, Jie and Cui, Ganqu and Zhang, Zhengyan and Yang, Cheng and Liu, Zhiyuan and Wang, Lifeng and Li, Changcheng and Sun, Maosong},
	month = dec,
	year = {2018},
	note = {arXiv: 1812.08434},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	file = {arXiv\:1812.08434 PDF:/Users/yl/Zotero/storage/89SH6GZW/Zhou et al. - 2018 - Graph Neural Networks A Review of Methods and App.pdf:application/pdf;arXiv.org Snapshot:/Users/yl/Zotero/storage/8AEE92DV/1812.html:text/html}
}

@article{velickovic_graph_2017,
	title = {Graph {Attention} {Networks}},
	url = {http://arxiv.org/abs/1710.10903},
	abstract = {We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).},
	urldate = {2019-08-05},
	journal = {arXiv:1710.10903 [cs, stat]},
	author = {Veličković, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Liò, Pietro and Bengio, Yoshua},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.10903},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning, Computer Science - Social and Information Networks},
	file = {arXiv\:1710.10903 PDF:/Users/yl/Zotero/storage/RGWUXXS8/Veličković et al. - 2017 - Graph Attention Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/yl/Zotero/storage/98MRNU38/1710.html:text/html}
}

@article{li_gated_2015,
	title = {Gated {Graph} {Sequence} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1511.05493},
	abstract = {Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks (Scarselli et al., 2009), which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models (e.g., LSTMs) when the problem is graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and graph algorithm learning tasks. We then show it achieves state-of-the-art performance on a problem from program verification, in which subgraphs need to be matched to abstract data structures.},
	urldate = {2019-08-05},
	journal = {arXiv:1511.05493 [cs, stat]},
	author = {Li, Yujia and Tarlow, Daniel and Brockschmidt, Marc and Zemel, Richard},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.05493},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv\:1511.05493 PDF:/Users/yl/Zotero/storage/CSY8T63R/Li et al. - 2015 - Gated Graph Sequence Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/yl/Zotero/storage/LK2AENUS/1511.html:text/html}
}

@inproceedings{zhang_sentence-state_2018,
	address = {Melbourne, Australia},
	title = {Sentence-{State} {LSTM} for {Text} {Representation}},
	url = {https://www.aclweb.org/anthology/P18-1030},
	doi = {10.18653/v1/P18-1030},
	abstract = {Bi-directional LSTMs are a powerful tool for text representation. On the other hand, they have been shown to suffer various limitations due to their sequential nature. We investigate an alternative LSTM structure for encoding text, which consists of a parallel state for each word. Recurrent steps are used to perform local and global information exchange between words simultaneously, rather than incremental reading of a sequence of words. Results on various classification and sequence labelling benchmarks show that the proposed model has strong representation power, giving highly competitive performances compared to stacked BiLSTM models with similar parameter numbers.},
	urldate = {2019-08-05},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Yue and Liu, Qi and Song, Linfeng},
	month = jul,
	year = {2018},
	pages = {317--327},
	file = {Full Text PDF:/Users/yl/Zotero/storage/KFXJCQXQ/Zhang et al. - 2018 - Sentence-State LSTM for Text Representation.pdf:application/pdf}
}

@inproceedings{song_graph--sequence_2018,
	address = {Melbourne, Australia},
	title = {A {Graph}-to-{Sequence} {Model} for {AMR}-to-{Text} {Generation}},
	url = {https://www.aclweb.org/anthology/P18-1150},
	doi = {10.18653/v1/P18-1150},
	abstract = {The problem of AMR-to-text generation is to recover a text representing the same meaning as an input AMR graph. The current state-of-the-art method uses a sequence-to-sequence model, leveraging LSTM for encoding a linearized AMR structure. Although being able to model non-local semantic information, a sequence LSTM can lose information from the AMR graph structure, and thus facing challenges with large-graphs, which result in long sequences. We introduce a neural graph-to-sequence model, using a novel LSTM structure for directly encoding graph-level semantics. On a standard benchmark, our model shows superior results to existing methods in the literature.},
	urldate = {2019-08-05},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Song, Linfeng and Zhang, Yue and Wang, Zhiguo and Gildea, Daniel},
	month = jul,
	year = {2018},
	pages = {1616--1626},
	file = {Full Text PDF:/Users/yl/Zotero/storage/3DQVSKRK/Song et al. - 2018 - A Graph-to-Sequence Model for AMR-to-Text Generati.pdf:application/pdf}
}

@book{bishop_pattern_2006,
	address = {New York, NY},
	series = {Information science and statistics},
	title = {Pattern recognition and machine learning},
	isbn = {978-0-387-31073-2 978-1-4939-3843-8},
	abstract = {This is the first textbook on pattern recognition to present the Bayesian viewpoint. The book presents approximate inference algorithms that permit fast approximate answers in situations where exact answers are not feasible. It uses graphical models to describe probability distributions when no other books apply graphical models to machine learning. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory},
	language = {eng},
	publisher = {Springer},
	author = {Bishop, Christopher M.},
	year = {2006},
	keywords = {Bayesian networks, Computing and Computers, neural networks, sparse kernel machines}
}

@book{freedman_statistical_2009,
	title = {Statistical {Models}: {Theory} and {Practice}},
	isbn = {978-1-139-47731-4},
	shorttitle = {Statistical {Models}},
	abstract = {This lively and engaging book explains the things you have to know in order to read empirical papers in the social and health sciences, as well as the techniques you need to build statistical models of your own. The discussion in the book is organized around published studies, as are many of the exercises. Relevant journal articles are reprinted at the back of the book. Freedman makes a thorough appraisal of the statistical methods in these papers and in a variety of other examples. He illustrates the principles of modelling, and the pitfalls. The discussion shows you how to think about the critical issues - including the connection (or lack of it) between the statistical models and the real phenomena. The book is written for advanced undergraduates and beginning graduate students in statistics, as well as students and professionals in the social and health sciences.},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Freedman, David A.},
	month = apr,
	year = {2009},
	note = {Google-Books-ID: fW\_9BV5Wpf8C},
	keywords = {Mathematics / Probability \& Statistics / General}
}

@book{barber_bayesian_2012,
	title = {Bayesian {Reasoning} and {Machine} {Learning}},
	isbn = {978-0-521-51814-7},
	abstract = {Machine learning methods extract value from vast data sets quickly and with modest resources. They are established tools in a wide range of industrial applications, including search engines, DNA sequencing, stock market analysis, and robot locomotion, and their use is spreading rapidly. People who know the methods have their choice of rewarding jobs. This hands-on text opens these opportunities to computer science students with modest mathematical backgrounds. It is designed for final-year undergraduates and master's students with limited background in linear algebra and calculus. Comprehensive and coherent, it develops everything from basic reasoning to advanced techniques within the framework of graphical models. Students learn more than a menu of techniques, they develop analytical and problem-solving skills that equip them for the real world. Numerous examples and exercises, both computer based and theoretical, are included in every chapter. Resources for students and instructors, including a MATLAB toolbox, are available online.},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Barber, David},
	month = feb,
	year = {2012},
	note = {Google-Books-ID: yxZtddB\_Ob0C},
	keywords = {Mathematics / Probability \& Statistics / General, Computers / Computer Vision \& Pattern Recognition, Computers / Intelligence (AI) \& Semantics}
}

@incollection{rasmussen_gaussian_2004,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Gaussian {Processes} in {Machine} {Learning}},
	isbn = {978-3-540-28650-9},
	url = {https://doi.org/10.1007/978-3-540-28650-9_4},
	abstract = {We give a basic introduction to Gaussian Process regression models. We focus on understanding the role of the stochastic process and how it is used to define a distribution over functions. We present the simple equations for incorporating training data and examine how to learn the hyperparameters using the marginal likelihood. We explain the practical advantages of Gaussian Process and end with conclusions and a look at the current trends in GP work.},
	language = {en},
	urldate = {2019-08-05},
	booktitle = {Advanced {Lectures} on {Machine} {Learning}: {ML} {Summer} {Schools} 2003, {Canberra}, {Australia}, {February} 2 - 14, 2003, {Tübingen}, {Germany}, {August} 4 - 16, 2003, {Revised} {Lectures}},
	publisher = {Springer Berlin Heidelberg},
	author = {Rasmussen, Carl Edward},
	editor = {Bousquet, Olivier and von Luxburg, Ulrike and Rätsch, Gunnar},
	year = {2004},
	doi = {10.1007/978-3-540-28650-9_4},
	keywords = {Covariance Function, Gaussian Process, Joint Gaussian Distribution, Marginal Likelihood, Posterior Variance},
	pages = {63--71},
	file = {Submitted Version:/Users/yl/Zotero/storage/2ZCS2QRW/Rasmussen - 2004 - Gaussian Processes in Machine Learning.pdf:application/pdf}
}

@book{jurafsky_speech_2000,
	address = {Upper Saddle River, NJ, USA},
	edition = {1st},
	title = {Speech and {Language} {Processing}: {An} {Introduction} to {Natural} {Language} {Processing}, {Computational} {Linguistics}, and {Speech} {Recognition}},
	isbn = {978-0-13-095069-7},
	shorttitle = {Speech and {Language} {Processing}},
	abstract = {From the Publisher:This book takes an empirical approach to language processing, based on applying statistical and other machine-learning algorithms to large corpora.Methodology boxes are included in each chapter. Each chapter is built around one or more worked examples to demonstrate the main idea of the chapter. Covers the fundamental algorithms of various fields, whether originally proposed for spoken or written language to demonstrate how the same algorithm can be used for speech recognition and word-sense disambiguation. Emphasis on web and other practical applications. Emphasis on scientific evaluation. Useful as a reference for professionals in any of the areas of speech and language processing.},
	publisher = {Prentice Hall PTR},
	author = {Jurafsky, Daniel and Martin, James H.},
	year = {2000}
}

@article{rumelhart_learning_1986,
	title = {Learning representations by back-propagating errors},
	volume = {323},
	copyright = {1986 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/323533a0},
	doi = {10.1038/323533a0},
	abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
	language = {en},
	number = {6088},
	urldate = {2019-08-05},
	journal = {Nature},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	month = oct,
	year = {1986},
	pages = {533--536},
	file = {Snapshot:/Users/yl/Zotero/storage/RAAI8F2N/323533a0.html:text/html}
}

@article{turing_i.computing_1950,
	title = {I.—{COMPUTING} {MACHINERY} {AND} {INTELLIGENCE}},
	volume = {LIX},
	issn = {0026-4423},
	url = {https://academic.oup.com/mind/article/LIX/236/433/986238},
	doi = {10.1093/mind/LIX.236.433},
	abstract = {A. M. TURING;  I.—COMPUTING MACHINERY AND INTELLIGENCE, Mind, Volume LIX, Issue 236, 1 October 1950, Pages 433–460, https://doi.org/10.1093/mind/LIX.236.433},
	language = {en},
	number = {236},
	urldate = {2019-08-05},
	journal = {Mind},
	author = {Turing, A. M.},
	month = oct,
	year = {1950},
	pages = {433--460},
	file = {Full Text PDF:/Users/yl/Zotero/storage/VSCG5J23/Turing - 1950 - I.—COMPUTING MACHINERY AND INTELLIGENCE.pdf:application/pdf;Snapshot:/Users/yl/Zotero/storage/EHAMZXCB/986238.html:text/html}
}

@inproceedings{strubell_fast_2017,
	address = {Copenhagen, Denmark},
	title = {Fast and {Accurate} {Entity} {Recognition} with {Iterated} {Dilated} {Convolutions}},
	url = {https://www.aclweb.org/anthology/D17-1283},
	doi = {10.18653/v1/D17-1283},
	abstract = {Today when many practitioners run basic NLP on the entire web and large-volume traffic, faster methods are paramount to saving time and energy costs. Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining per-token vector representations serving as input to labeling tasks such as NER (often followed by prediction in a linear-chain CRF). Though expressive and accurate, these models fail to fully exploit GPU parallelism, limiting their computational efficiency. This paper proposes a faster alternative to Bi-LSTMs for NER: Iterated Dilated Convolutional Neural Networks (ID-CNNs), which have better capacity than traditional CNNs for large context and structured prediction. Unlike LSTMs whose sequential processing on sentences of length N requires O(N) time even in the face of parallelism, ID-CNNs permit fixed-depth convolutions to run in parallel across entire documents. We describe a distinct combination of network structure, parameter sharing and training procedures that enable dramatic 14-20x test-time speedups while retaining accuracy comparable to the Bi-LSTM-CRF. Moreover, ID-CNNs trained to aggregate context from the entire document are more accurate than Bi-LSTM-CRFs while attaining 8x faster test time speeds.},
	urldate = {2019-08-06},
	booktitle = {Proceedings of the 2017 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Strubell, Emma and Verga, Patrick and Belanger, David and McCallum, Andrew},
	month = sep,
	year = {2017},
	pages = {2670--2680},
	file = {Full Text PDF:/Users/yl/Zotero/storage/E3G2JX9E/Strubell et al. - 2017 - Fast and Accurate Entity Recognition with Iterated.pdf:application/pdf}
}

@inproceedings{wiseman_learning_2016,
	address = {San Diego, California},
	title = {Learning {Global} {Features} for {Coreference} {Resolution}},
	url = {https://www.aclweb.org/anthology/N16-1114},
	doi = {10.18653/v1/N16-1114},
	urldate = {2019-08-06},
	booktitle = {Proceedings of the 2016 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Wiseman, Sam and Rush, Alexander M. and Shieber, Stuart M.},
	month = jun,
	year = {2016},
	pages = {994--1004},
	file = {Full Text PDF:/Users/yl/Zotero/storage/W7RYWVQT/Wiseman et al. - 2016 - Learning Global Features for Coreference Resolutio.pdf:application/pdf}
}

@inproceedings{clark_deep_2016,
	address = {Austin, Texas},
	title = {Deep {Reinforcement} {Learning} for {Mention}-{Ranking} {Coreference} {Models}},
	url = {https://www.aclweb.org/anthology/D16-1245},
	doi = {10.18653/v1/D16-1245},
	urldate = {2019-08-06},
	booktitle = {Proceedings of the 2016 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Clark, Kevin and Manning, Christopher D.},
	month = nov,
	year = {2016},
	pages = {2256--2262},
	file = {Full Text PDF:/Users/yl/Zotero/storage/VCQYCWUK/Clark and Manning - 2016 - Deep Reinforcement Learning for Mention-Ranking Co.pdf:application/pdf}
}

@article{martschat_latent_2015,
	title = {Latent {Structures} for {Coreference} {Resolution}},
	volume = {3},
	url = {https://www.aclweb.org/anthology/Q15-1029},
	doi = {10.1162/tacl_a_00147},
	abstract = {Machine learning approaches to coreference resolution vary greatly in the modeling of the problem: while early approaches operated on the mention pair level, current research focuses on ranking architectures and antecedent trees. We propose a unified representation of different approaches to coreference resolution in terms of the structure they operate on. We represent several coreference resolution approaches proposed in the literature in our framework and evaluate their performance. Finally, we conduct a systematic analysis of the output of these approaches, highlighting differences and similarities.},
	urldate = {2019-08-06},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Martschat, Sebastian and Strube, Michael},
	year = {2015},
	pages = {405--418},
	file = {Full Text PDF:/Users/yl/Zotero/storage/7G5CANKW/Martschat and Strube - 2015 - Latent Structures for Coreference Resolution.pdf:application/pdf}
}

@inproceedings{andrenucci_automated_2005,
	title = {Automated question answering: review of the main approaches},
	volume = {1},
	shorttitle = {Automated question answering},
	doi = {10.1109/ICITA.2005.78},
	abstract = {Automated question-answering aims at delivering concise information that contains answers to user questions. This paper reviews and compares three main question-answering approaches based on natural language processing, information retrieval and question templates, eliciting their differences and the context of application that best suits each of them.},
	booktitle = {Third {International} {Conference} on {Information} {Technology} and {Applications} ({ICITA}'05)},
	author = {Andrenucci, A. and Sneiders, E.},
	month = jul,
	year = {2005},
	keywords = {automated question answering system, Data mining, information retrieval, Information retrieval, Logic, natural language processing, Natural language processing, natural languages, Natural languages, Problem-solving, question templates, Search engines, Spatial databases, Text recognition, World Wide Web},
	pages = {514--519 vol.1},
	file = {IEEE Xplore Abstract Record:/Users/yl/Zotero/storage/BX7S5FY3/1488857.html:text/html}
}

@inproceedings{van_der_malsburg_frank_1986,
	title = {Frank {Rosenblatt}: {Principles} of {Neurodynamics}: {Perceptrons} and the {Theory} of {Brain} {Mechanisms}},
	isbn = {978-3-642-70911-1},
	shorttitle = {Frank {Rosenblatt}},
	abstract = {Frank Rosenblatt’s intention with his book, according to his own introduction, is not just to describe a machine, the perceptron, but rather to put forward a theory. He formulates a series of machines. Each machine serves to introduce a new concept.},
	language = {en},
	booktitle = {Brain {Theory}},
	publisher = {Springer Berlin Heidelberg},
	author = {Van Der Malsburg, C.},
	editor = {Palm, Günther and Aertsen, Ad},
	year = {1986},
	keywords = {Brain Theory, Small Pattern, Synaptic Plasticity, Synaptic Weight, Syntactical Information},
	pages = {245--248}
}

@inproceedings{pascanu_difficulty_2013,
	series = {{ICML}'13},
	title = {On the {Difficulty} of {Training} {Recurrent} {Neural} {Networks}},
	url = {http://dl.acm.org/citation.cfm?id=3042817.3043083},
	abstract = {There are two widely known issues with properly training recurrent neural networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
	urldate = {2019-08-06},
	booktitle = {Proceedings of the 30th {International} {Conference} on {International} {Conference} on {Machine} {Learning} - {Volume} 28},
	publisher = {JMLR.org},
	author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
	year = {2013},
	note = {event-place: Atlanta, GA, USA},
	pages = {III--1310--III--1318}
}

@article{elman_finding_1990,
	title = {Finding {Structure} in {Time}},
	volume = {14},
	copyright = {© 1990 Cognitive Science Society, Inc.},
	issn = {1551-6709},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1},
	doi = {10.1207/s15516709cog1402_1},
	abstract = {Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves: the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands: indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent, while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction.},
	language = {en},
	number = {2},
	urldate = {2019-08-06},
	journal = {Cognitive Science},
	author = {Elman, Jeffrey L.},
	year = {1990},
	pages = {179--211},
	file = {Full Text PDF:/Users/yl/Zotero/storage/YP4NETEI/Elman - 1990 - Finding Structure in Time.pdf:application/pdf}
}

@article{hochreiter_long_1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	issn = {0899-7667},
	url = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	number = {8},
	urldate = {2019-08-06},
	journal = {Neural Comput.},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	month = nov,
	year = {1997},
	pages = {1735--1780}
}

@inproceedings{graves_speech_2013,
	title = {Speech recognition with deep recurrent neural networks},
	doi = {10.1109/ICASSP.2013.6638947},
	abstract = {Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates deep recurrent neural networks, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7\% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.},
	booktitle = {2013 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing}},
	author = {Graves, A. and Mohamed, A. and Hinton, G.},
	month = may,
	year = {2013},
	keywords = {Acoustics, connectionist temporal classification, deep neural networks, deep recurrent neural networks, end-to-end training methods, long short-term memory RNN architecture, Noise, recurrent neural networks, Recurrent neural networks, sequential data, speech recognition, Speech recognition, Training, Vectors},
	pages = {6645--6649},
	file = {IEEE Xplore Abstract Record:/Users/yl/Zotero/storage/FEZGHKXC/6638947.html:text/html}
}

@incollection{wilson_marginal_2017,
	title = {The {Marginal} {Value} of {Adaptive} {Gradient} {Methods} in {Machine} {Learning}},
	url = {http://papers.nips.cc/paper/7003-the-marginal-value-of-adaptive-gradient-methods-in-machine-learning.pdf},
	urldate = {2019-08-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 30},
	publisher = {Curran Associates, Inc.},
	author = {Wilson, Ashia C and Roelofs, Rebecca and Stern, Mitchell and Srebro, Nati and Recht, Benjamin},
	editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
	pages = {4148--4158},
	file = {NIPS Full Text PDF:/Users/yl/Zotero/storage/H4VPETY6/Wilson et al. - 2017 - The Marginal Value of Adaptive Gradient Methods in.pdf:application/pdf;NIPS Snapshot:/Users/yl/Zotero/storage/KN33NEXQ/7003-the-marginal-value-of-adaptive-gradient-methods-in-machine-learning.html:text/html}
}

@inproceedings{angeline_evolutionary_1998,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Evolutionary optimization versus particle swarm optimization: {Philosophy} and performance differences},
	isbn = {978-3-540-68515-9},
	shorttitle = {Evolutionary optimization versus particle swarm optimization},
	abstract = {This paper investigates the philosophical and performance differences of particle swarm and evolutionary optimization. The method of processing employed in each technique are first reviewed followed by a summary of their philosophical differences. Comparison experiments involving four non-linear functions well studied in the evolutionary optimization literature are used to highlight some performance differences between the techniques.},
	language = {en},
	booktitle = {Evolutionary {Programming} {VII}},
	publisher = {Springer Berlin Heidelberg},
	author = {Angeline, Peter J.},
	editor = {Porto, V. W. and Saravanan, N. and Waagen, D. and Eiben, A. E.},
	year = {1998},
	keywords = {Evolutionary Computation, Evolutionary Optimization, Particle Swarm, Particle Swarm Optimization, Strategy Parameter},
	pages = {601--610},
	file = {Springer Full Text PDF:/Users/yl/Zotero/storage/KVDFTTK4/Angeline - 1998 - Evolutionary optimization versus particle swarm op.pdf:application/pdf}
}

@inproceedings{kennedy_particle_1995,
	title = {Particle swarm optimization},
	volume = {4},
	doi = {10.1109/ICNN.1995.488968},
	abstract = {A concept for the optimization of nonlinear functions using particle swarm methodology is introduced. The evolution of several paradigms is outlined, and an implementation of one of the paradigms is discussed. Benchmark testing of the paradigm is described, and applications, including nonlinear function optimization and neural network training, are proposed. The relationships between particle swarm optimization and both artificial life and genetic algorithms are described.},
	booktitle = {Proceedings of {ICNN}'95 - {International} {Conference} on {Neural} {Networks}},
	author = {Kennedy, J. and Eberhart, R.},
	month = nov,
	year = {1995},
	keywords = {Testing, artificial intelligence, artificial life, Artificial neural networks, Birds, Educational institutions, evolution, genetic algorithms, Genetic algorithms, Humans, Marine animals, multidimensional search, neural nets, neural network, nonlinear functions, optimization, Optimization methods, particle swarm, Particle swarm optimization, Performance evaluation, search problems, simulation, social metaphor},
	pages = {1942--1948 vol.4},
	file = {IEEE Xplore Abstract Record:/Users/yl/Zotero/storage/MXY63FXR/488968.html:text/html}
}

@inproceedings{dumais_using_1988,
	address = {New York, NY, USA},
	series = {{CHI} '88},
	title = {Using {Latent} {Semantic} {Analysis} to {Improve} {Access} to {Textual} {Information}},
	isbn = {978-0-201-14237-2},
	url = {http://doi.acm.org/10.1145/57167.57214},
	doi = {10.1145/57167.57214},
	abstract = {This paper describes a new approach for dealing with the vocabulary problem in human-computer interaction. Most approaches to retrieving textual materials depend on a lexical match between words in users' requests and those in or assigned to database objects. Because of the tremendous diversity in the words people use to describe the same object, lexical matching methods are necessarily incomplete and imprecise [5]. The latent semantic indexing approach tries to overcome these problems by automatically organizing text objects into a semantic structure more appropriate for matching user requests. This is done by taking advantage of implicit higher-order structure in the association of terms with text objects. The particular technique used is singular-value decomposition, in which a large term by text-object matrix is decomposed into a set of about 50 to 150 orthogonal factors from which the original matrix can be approximated by linear combination. Terms and objects are represented by 50 to 150 dimensional vectors and matched against user queries in this “semantic” space. Initial tests find this completely automatic method widely applicable and a promising way to improve users' access to many kinds of textual materials, or to objects and services for which textual descriptions are available.},
	urldate = {2019-08-07},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Dumais, S. T. and Furnas, G. W. and Landauer, T. K. and Deerwester, S. and Harshman, R.},
	year = {1988},
	note = {event-place: Washington, D.C., USA},
	pages = {281--285},
	file = {ACM Full Text PDF:/Users/yl/Zotero/storage/FLBTNA5G/Dumais et al. - 1988 - Using Latent Semantic Analysis to Improve Access t.pdf:application/pdf}
}

@incollection{mikolov_distributed_2013,
	title = {Distributed {Representations} of {Words} and {Phrases} and their {Compositionality}},
	url = {http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf},
	urldate = {2019-08-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 26},
	publisher = {Curran Associates, Inc.},
	author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
	editor = {Burges, C. J. C. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
	year = {2013},
	pages = {3111--3119},
	file = {NIPS Full Text PDF:/Users/yl/Zotero/storage/K7U8XX8P/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases a.pdf:application/pdf;NIPS Snapshot:/Users/yl/Zotero/storage/L48K94RA/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.html:text/html}
}

@article{scarselli_graph_2009,
	title = {The {Graph} {Neural} {Network} {Model}},
	volume = {20},
	issn = {1045-9227},
	doi = {10.1109/TNN.2008.2005605},
	abstract = {Many underlying relationships among data in several areas of science and engineering, e.g., computer vision, molecular chemistry, molecular biology, pattern recognition, and data mining, can be represented in terms of graphs. In this paper, we propose a new neural network model, called graph neural network (GNN) model, that extends existing neural network methods for processing the data represented in graph domains. This GNN model, which can directly process most of the practically useful types of graphs, e.g., acyclic, cyclic, directed, and undirected, implements a function tau(G,n) isin IRm that maps a graph G and one of its nodes n into an m-dimensional Euclidean space. A supervised learning algorithm is derived to estimate the parameters of the proposed GNN model. The computational cost of the proposed algorithm is also considered. Some experimental results are shown to validate the proposed learning algorithm, and to demonstrate its generalization capabilities.},
	number = {1},
	journal = {IEEE Transactions on Neural Networks},
	author = {Scarselli, F. and Gori, M. and Tsoi, A. C. and Hagenbuchner, M. and Monfardini, G.},
	month = jan,
	year = {2009},
	keywords = {Data mining, neural nets, acyclic graph, Algorithms, Artificial Intelligence, Biological system modeling, Biology, Chemistry, computer vision, Computer vision, cyclic graph, Data engineering, data mining, Databases, Factual, directed graph, graph neural network model, graph neural networks (GNNs), graph processing, graph theory, Graphical domains, Internet, learning (artificial intelligence), Linear Models, m-dimensional Euclidean space, molecular biology, molecular chemistry, Neural networks, Neural Networks (Computer), Nonlinear Dynamics, parameter estimation, Parameter estimation, pattern recognition, Pattern recognition, Pattern Recognition, Automated, recursive neural networks, Regression Analysis, Reproducibility of Results, Supervised learning, supervised learning algorithm, undirected graph},
	pages = {61--80},
	file = {Full Text:/Users/yl/Zotero/storage/JZEBS36P/Scarselli et al. - 2009 - The Graph Neural Network Model.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/yl/Zotero/storage/TAZY3UUP/4700287.html:text/html}
}

@article{robbins_stochastic_1951,
	title = {A {Stochastic} {Approximation} {Method}},
	volume = {22},
	issn = {0003-4851},
	url = {https://www.jstor.org/stable/2236626},
	abstract = {Let M(x) denote the expected value at level x of the response to a certain experiment. M(x) is assumed to be a monotone function of x but is unknown to the experimenter, and it is desired to find the solution x = θ of the equation M(x) = α, where α is a given constant. We give a method for making successive experiments at levels x1,x2,⋯ in such a way that xn will tend to θ in probability.},
	number = {3},
	urldate = {2019-08-07},
	journal = {The Annals of Mathematical Statistics},
	author = {Robbins, Herbert and Monro, Sutton},
	year = {1951},
	pages = {400--407}
}

@article{kingma_adam:_2014,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2019-08-07},
	journal = {arXiv:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.6980},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv\:1412.6980 PDF:/Users/yl/Zotero/storage/U3A2RPZX/Kingma and Ba - 2014 - Adam A Method for Stochastic Optimization.pdf:application/pdf;arXiv.org Snapshot:/Users/yl/Zotero/storage/UL8RDQUL/1412.html:text/html}
}

@article{yang_embedding_2014,
	title = {Embedding {Entities} and {Relations} for {Learning} and {Inference} in {Knowledge} {Bases}},
	url = {http://arxiv.org/abs/1412.6575},
	abstract = {We consider learning representations of entities and relations in KBs using the neural-embedding approach. We show that most existing models, including NTN (Socher et al., 2013) and TransE (Bordes et al., 2013b), can be generalized under a unified learning framework, where entities are low-dimensional vectors learned from a neural network and relations are bilinear and/or linear mapping functions. Under this framework, we compare a variety of embedding models on the link prediction task. We show that a simple bilinear formulation achieves new state-of-the-art results for the task (achieving a top-10 accuracy of 73.2\% vs. 54.7\% by TransE on Freebase). Furthermore, we introduce a novel approach that utilizes the learned relation embeddings to mine logical rules such as "BornInCity(a,b) and CityInCountry(b,c) ={\textgreater} Nationality(a,c)". We find that embeddings learned from the bilinear objective are particularly good at capturing relational semantics and that the composition of relations is characterized by matrix multiplication. More interestingly, we demonstrate that our embedding-based rule extraction approach successfully outperforms a state-of-the-art confidence-based rule mining approach in mining Horn rules that involve compositional reasoning.},
	urldate = {2019-08-09},
	journal = {arXiv:1412.6575 [cs]},
	author = {Yang, Bishan and Yih, Wen-tau and He, Xiaodong and Gao, Jianfeng and Deng, Li},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.6575},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv\:1412.6575 PDF:/Users/yl/Zotero/storage/AP24FEDX/Yang et al. - 2014 - Embedding Entities and Relations for Learning and .pdf:application/pdf;arXiv.org Snapshot:/Users/yl/Zotero/storage/KCGVZDDG/1412.html:text/html}
}

@article{li_deepgcns:_2019,
	title = {{DeepGCNs}: {Can} {GCNs} {Go} as {Deep} as {CNNs}?},
	shorttitle = {{DeepGCNs}},
	url = {http://arxiv.org/abs/1904.03751},
	abstract = {Convolutional Neural Networks (CNNs) achieve impressive performance in a wide variety of fields. Their success benefited from a massive boost when very deep CNN models were able to be reliably trained. Despite their merits, CNNs fail to properly address problems with non-Euclidean data. To overcome this challenge, Graph Convolutional Networks (GCNs) build graphs to represent non-Euclidean data, borrow concepts from CNNs, and apply them in training. GCNs show promising results, but they are usually limited to very shallow models due to the vanishing gradient problem. As a result, most state-of-the-art GCN models are no deeper than 3 or 4 layers. In this work, we present new ways to successfully train very deep GCNs. We do this by borrowing concepts from CNNs, specifically residual/dense connections and dilated convolutions, and adapting them to GCN architectures. Extensive experiments show the positive effect of these deep GCN frameworks. Finally, we use these new concepts to build a very deep 56-layer GCN, and show how it significantly boosts performance (+3.7\% mIoU over state-of-the-art) in the task of point cloud semantic segmentation. We believe that the community can greatly benefit from this work, as it opens up many opportunities for advancing GCN-based research.},
	urldate = {2019-08-21},
	journal = {arXiv:1904.03751 [cs]},
	author = {Li, Guohao and Müller, Matthias and Thabet, Ali and Ghanem, Bernard},
	month = apr,
	year = {2019},
	note = {arXiv: 1904.03751},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1904.03751 PDF:/Users/yl/Zotero/storage/W63XRY7I/Li et al. - 2019 - DeepGCNs Can GCNs Go as Deep as CNNs.pdf:application/pdf;arXiv.org Snapshot:/Users/yl/Zotero/storage/TB58N5GN/1904.html:text/html}
}

@article{hammond_wavelets_2011,
	title = {Wavelets on graphs via spectral graph theory},
	volume = {30},
	issn = {1063-5203},
	url = {http://www.sciencedirect.com/science/article/pii/S1063520310000552},
	doi = {10.1016/j.acha.2010.04.005},
	abstract = {We propose a novel method for constructing wavelet transforms of functions defined on the vertices of an arbitrary finite weighted graph. Our approach is based on defining scaling using the graph analogue of the Fourier domain, namely the spectral decomposition of the discrete graph Laplacian L. Given a wavelet generating kernel g and a scale parameter t, we define the scaled wavelet operator Tgt=g(tL). The spectral graph wavelets are then formed by localizing this operator by applying it to an indicator function. Subject to an admissibility condition on g, this procedure defines an invertible transform. We explore the localization properties of the wavelets in the limit of fine scales. Additionally, we present a fast Chebyshev polynomial approximation algorithm for computing the transform that avoids the need for diagonalizing L. We highlight potential applications of the transform through examples of wavelets on graphs corresponding to a variety of different problem domains.},
	number = {2},
	urldate = {2019-08-22},
	journal = {Applied and Computational Harmonic Analysis},
	author = {Hammond, David K. and Vandergheynst, Pierre and Gribonval, Rémi},
	month = mar,
	year = {2011},
	keywords = {Graph theory, Overcomplete wavelet frames, Spectral graph theory, Wavelets},
	pages = {129--150},
	file = {ScienceDirect Full Text PDF:/Users/yl/Zotero/storage/IW3X7R7I/Hammond et al. - 2011 - Wavelets on graphs via spectral graph theory.pdf:application/pdf;ScienceDirect Snapshot:/Users/yl/Zotero/storage/REVNU9YS/S1063520310000552.html:text/html}
}

@article{scargle_studies_2013,
	title = {{STUDIES} {IN} {ASTRONOMICAL} {TIME} {SERIES} {ANALYSIS}. {VI}. {BAYESIAN} {BLOCK} {REPRESENTATIONS}},
	volume = {764},
	issn = {0004-637X},
	url = {https://doi.org/10.1088%2F0004-637x%2F764%2F2%2F167},
	doi = {10.1088/0004-637X/764/2/167},
	abstract = {This paper addresses the problem of detecting and characterizing local variability in time series and other forms of sequential data. The goal is to identify and characterize statistically significant variations, at the same time suppressing the inevitable corrupting observational errors. We present a simple nonparametric modeling technique and an algorithm implementing it—an improved and generalized version of Bayesian Blocks—that finds the optimal segmentation of the data in the observation interval. The structure of the algorithm allows it to be used in either a real-time trigger mode, or a retrospective mode. Maximum likelihood or marginal posterior functions to measure model fitness are presented for events, binned counts, and measurements at arbitrary times with known error distributions. Problems addressed include those connected with data gaps, variable exposure, extension to piecewise linear and piecewise exponential representations, multivariate time series data, analysis of variance, data on the circle, other data modes, and dispersed data. Simulations provide evidence that the detection efficiency for weak signals is close to a theoretical asymptotic limit derived by Arias-Castro et al. In the spirit of Reproducible Research all of the code and data necessary to reproduce all of the figures in this paper are included as supplementary material.},
	language = {en},
	number = {2},
	urldate = {2019-08-22},
	journal = {ApJ},
	author = {Scargle, Jeffrey D. and Norris, Jay P. and Jackson, Brad and Chiang, James},
	month = feb,
	year = {2013},
	pages = {167},
	file = {IOP Full Text PDF:/Users/yl/Zotero/storage/SU2G9NHM/Scargle et al. - 2013 - STUDIES IN ASTRONOMICAL TIME SERIES ANALYSIS. VI. .pdf:application/pdf}
}

@inproceedings{defferrard_convolutional_2016,
	address = {USA},
	series = {{NIPS}'16},
	title = {Convolutional {Neural} {Networks} on {Graphs} with {Fast} {Localized} {Spectral} {Filtering}},
	isbn = {978-1-5108-3881-9},
	url = {http://dl.acm.org/citation.cfm?id=3157382.3157527},
	abstract = {In this work, we are interested in generalizing convolutional neural networks (CNNs) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, brain connectomes or words' embedding, represented by graphs. We present a formulation of CNNs in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs. Importantly, the proposed technique offers the same linear computational complexity and constant learning complexity as classical CNNs, while being universal to any graph structure. Experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs.},
	urldate = {2019-08-23},
	booktitle = {Proceedings of the 30th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Defferrard, Michaël and Bresson, Xavier and Vandergheynst, Pierre},
	year = {2016},
	note = {event-place: Barcelona, Spain},
	pages = {3844--3852},
	file = {ACM Full Text PDF:/Users/yl/Zotero/storage/HGZZVWCD/Defferrard et al. - 2016 - Convolutional Neural Networks on Graphs with Fast .pdf:application/pdf}
}

@incollection{hamilton_inductive_2017,
	title = {Inductive {Representation} {Learning} on {Large} {Graphs}},
	url = {http://papers.nips.cc/paper/6703-inductive-representation-learning-on-large-graphs.pdf},
	urldate = {2019-08-23},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 30},
	publisher = {Curran Associates, Inc.},
	author = {Hamilton, Will and Ying, Zhitao and Leskovec, Jure},
	editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
	pages = {1024--1034},
	file = {NIPS Full Text PDF:/Users/yl/Zotero/storage/HCWLJAPQ/Hamilton et al. - 2017 - Inductive Representation Learning on Large Graphs.pdf:application/pdf;NIPS Snapshot:/Users/yl/Zotero/storage/4ILW3WF6/6703-inductive-representation-learning-on-large-graphs.html:text/html}
}

@article{nickel_review_2016,
	title = {A {Review} of {Relational} {Machine} {Learning} for {Knowledge} {Graphs}},
	volume = {104},
	issn = {0018-9219},
	doi = {10.1109/JPROC.2015.2483592},
	abstract = {Relational machine learning studies methods for the statistical analysis of relational, or graph-structured, data. In this paper, we provide a review of how such statistical models can be “trained” on large knowledge graphs, and then used to predict new facts about the world (which is equivalent to predicting new edges in the graph). In particular, we discuss two fundamentally different kinds of statistical relational models, both of which can scale to massive data sets. The first is based on latent feature models such as tensor factorization and multiway neural networks. The second is based on mining observable patterns in the graph. We also show how to combine these latent and observable models to get improved modeling power at decreased computational cost. Finally, we discuss how such statistical models of graphs can be combined with text-based information extraction methods for automatically constructing knowledge graphs from the Web. To this end, we also discuss Google's knowledge vault project as an example of such combination.},
	number = {1},
	journal = {Proceedings of the IEEE},
	author = {Nickel, M. and Murphy, K. and Tresp, V. and Gabrilovich, E.},
	month = jan,
	year = {2016},
	keywords = {Data mining, Biological system modeling, data mining, graph theory, learning (artificial intelligence), automatic knowledge graph construction, Big data, computational cost, Computer graphs, Google knowledge vault project, graph edges, Graph-based models, graph-structured data, Knowledge based systems, knowledge extraction, knowledge graphs, latent feature models, Machine learning, multiway neural networks, observable pattern mining, Predictive models, relational data, relational machine learning, Resource description framework, statistical analysis, Statistical analysis, statistical relational learning, statistical relational models, tensor factorization, text analysis, text-based information extraction methods},
	pages = {11--33},
	file = {IEEE Xplore Abstract Record:/Users/yl/Zotero/storage/W6RU7D6L/7358050.html:text/html;Submitted Version:/Users/yl/Zotero/storage/K5MZD3TG/Nickel et al. - 2016 - A Review of Relational Machine Learning for Knowle.pdf:application/pdf}
}

@incollection{bordes_translating_2013,
	title = {Translating {Embeddings} for {Modeling} {Multi}-relational {Data}},
	url = {http://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data.pdf},
	urldate = {2019-08-27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 26},
	publisher = {Curran Associates, Inc.},
	author = {Bordes, Antoine and Usunier, Nicolas and Garcia-Duran, Alberto and Weston, Jason and Yakhnenko, Oksana},
	editor = {Burges, C. J. C. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
	year = {2013},
	pages = {2787--2795},
	file = {NIPS Full Text PDF:/Users/yl/Zotero/storage/YT9UERH2/Bordes et al. - 2013 - Translating Embeddings for Modeling Multi-relation.pdf:application/pdf;NIPS Snapshot:/Users/yl/Zotero/storage/P29KJ26E/5071-translating-embeddings-for-modeling-multi-relational-data.html:text/html}
}

@article{adamic_friends_2003,
	title = {Friends and neighbors on the {Web}},
	volume = {25},
	issn = {0378-8733},
	url = {http://www.sciencedirect.com/science/article/pii/S0378873303000091},
	doi = {10.1016/S0378-8733(03)00009-1},
	abstract = {The Internet has become a rich and large repository of information about us as individuals. Anything from the links and text on a user’s homepage to the mailing lists the user subscribes to are reflections of social interactions a user has in the real world. In this paper we devise techniques and tools to mine this information in order to extract social networks and the exogenous factors underlying the networks’ structure. In an analysis of two data sets, from Stanford University and the Massachusetts Institute of Technology (MIT), we show that some factors are better indicators of social connections than others, and that these indicators vary between user populations. Our techniques provide potential applications in automatically inferring real world connections and discovering, labeling, and characterizing communities.},
	number = {3},
	urldate = {2019-08-27},
	journal = {Social Networks},
	author = {Adamic, Lada A and Adar, Eytan},
	month = jul,
	year = {2003},
	keywords = {Homepage analysis, Small worlds, Web communities},
	pages = {211--230},
	file = {ScienceDirect Snapshot:/Users/yl/Zotero/storage/H5BYSWSZ/S0378873303000091.html:text/html}
}

@article{berlusconi_link_2016,
	title = {Link {Prediction} in {Criminal} {Networks}: {A} {Tool} for {Criminal} {Intelligence} {Analysis}},
	volume = {11},
	issn = {1932-6203},
	shorttitle = {Link {Prediction} in {Criminal} {Networks}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4841537/},
	doi = {10.1371/journal.pone.0154244},
	abstract = {The problem of link prediction has recently received increasing attention from scholars in network science. In social network analysis, one of its aims is to recover missing links, namely connections among actors which are likely to exist but have not been reported because data are incomplete or subject to various types of uncertainty. In the field of criminal investigations, problems of incomplete information are encountered almost by definition, given the obvious anti-detection strategies set up by criminals and the limited investigative resources. In this paper, we work on a specific dataset obtained from a real investigation, and we propose a strategy to identify missing links in a criminal network on the basis of the topological analysis of the links classified as marginal, i.e. removed during the investigation procedure. The main assumption is that missing links should have opposite features with respect to marginal ones. Measures of node similarity turn out to provide the best characterization in this sense. The inspection of the judicial source documents confirms that the predicted links, in most instances, do relate actors with large likelihood of co-participation in illicit activities.},
	number = {4},
	urldate = {2019-08-27},
	journal = {PLoS One},
	author = {Berlusconi, Giulia and Calderoni, Francesco and Parolini, Nicola and Verani, Marco and Piccardi, Carlo},
	month = apr,
	year = {2016},
	pmid = {27104948},
	pmcid = {PMC4841537},
	file = {PubMed Central Full Text PDF:/Users/yl/Zotero/storage/GRV6Q9TG/Berlusconi et al. - 2016 - Link Prediction in Criminal Networks A Tool for C.pdf:application/pdf}
}

@inproceedings{talasu_link_2017,
	title = {A link prediction based approach for recommendation systems},
	doi = {10.1109/ICACCI.2017.8126148},
	abstract = {Recommendation Systems are an important tool for aiding discovery of content such as movies, books, and music. Generating personalised recommendations that deliver serendipitous suggestions to the user is a key factor in determining user satisfaction. In this paper, we propose a recommendation algorithm that uses a community-structure based link prediction approach. The proposed link prediction algorithm predicts the links that are most likely to be formed in the network based on two new proposed metrics. These metrics take into account the effects of both the network structure and the attribute information of the items for predicting probable links. The links predicted can be presented as recommendations to the user. An item network consisting of item associations derived from the attribute information usually displays a community structure with items of similar nature being clustered together. Our approach utilises this community structure to expose implicit links in the item network and provides non-obvious recommendations to the user.},
	booktitle = {2017 {International} {Conference} on {Advances} in {Computing}, {Communications} and {Informatics} ({ICACCI})},
	author = {Talasu, N. and Jonnalagadda, A. and Pillai, S. S. A. and Rahul, J.},
	month = sep,
	year = {2017},
	keywords = {data mining, Predictive models, attribute information, Collaboration, community structure, Community Structure, community-structure based link prediction approach, implicit links, information filtering, item associations, item network, Link Prediction, Measurement, network structure, Node Participation, Node Similarity, Peer-to-peer computing, personalised recommendations, Prediction algorithms, recommender systems, Recommender systems, Recommender Systems, Serendipity, social networking (online), user satisfaction},
	pages = {2059--2062},
	file = {IEEE Xplore Abstract Record:/Users/yl/Zotero/storage/YUVSMQUM/8126148.html:text/html}
}

@article{barabasi_emergence_1999,
	title = {Emergence of {Scaling} in {Random} {Networks}},
	volume = {286},
	issn = {0036-8075, 1095-9203},
	url = {https://science.sciencemag.org/content/286/5439/509},
	doi = {10.1126/science.286.5439.509},
	abstract = {Systems as diverse as genetic networks or the World Wide Web are best described as networks with complex topology. A common property of many large networks is that the vertex connectivities follow a scale-free power-law distribution. This feature was found to be a consequence of two generic mechanisms: (i) networks expand continuously by the addition of new vertices, and (ii) new vertices attach preferentially to sites that are already well connected. A model based on these two ingredients reproduces the observed stationary scale-free distributions, which indicates that the development of large networks is governed by robust self-organizing phenomena that go beyond the particulars of the individual systems.},
	language = {en},
	number = {5439},
	urldate = {2019-08-27},
	journal = {Science},
	author = {Barabási, Albert-László and Albert, Réka},
	month = oct,
	year = {1999},
	pmid = {10521342},
	pages = {509--512},
	file = {Full Text PDF:/Users/yl/Zotero/storage/56769EMY/Barabási and Albert - 1999 - Emergence of Scaling in Random Networks.pdf:application/pdf}
}

@article{zhou_predicting_2009,
	title = {Predicting missing links via local information},
	volume = {71},
	issn = {1434-6036},
	url = {https://doi.org/10.1140/epjb/e2009-00335-8},
	doi = {10.1140/epjb/e2009-00335-8},
	abstract = {Missing link prediction in networks is of both theoretical interest and practical significance in modern science. In this paper, we empirically investigate a simple framework of link prediction on the basis of node similarity. We compare nine well-known local similarity measures on six real networks. The results indicate that the simplest measure, namely Common Neighbours, has the best overall performance, and the Adamic-Adar index performs second best. A new similarity measure, motivated by the resource allocation process taking place on networks, is proposed and shown to have higher prediction accuracy than common neighbours. It is found that many links are assigned the same scores if only the information of the nearest neighbours is used. We therefore design another new measure exploiting information on the next nearest neighbours, which can remarkably enhance the prediction accuracy.},
	language = {en},
	number = {4},
	urldate = {2019-08-27},
	journal = {Eur. Phys. J. B},
	author = {Zhou, Tao and Lü, Linyuan and Zhang, Yi-Cheng},
	month = oct,
	year = {2009},
	keywords = {05.65.+b Self-organized systems, 89.75.-k Complex systems},
	pages = {623--630},
	file = {Springer Full Text PDF:/Users/yl/Zotero/storage/CCQ7MVNV/Zhou et al. - 2009 - Predicting missing links via local information.pdf:application/pdf}
}

@inproceedings{jeh_simrank:_2002,
	address = {New York, NY, USA},
	series = {{KDD} '02},
	title = {{SimRank}: {A} {Measure} of {Structural}-context {Similarity}},
	isbn = {978-1-58113-567-1},
	shorttitle = {{SimRank}},
	url = {http://doi.acm.org/10.1145/775047.775126},
	doi = {10.1145/775047.775126},
	abstract = {The problem of measuring "similarity" of objects arises in many applications, and many domain-specific measures have been developed, e.g., matching text across documents or computing overlap among item-sets. We propose a complementary approach, applicable in any domain with object-to-object relationships, that measures similarity of the structural context in which objects occur, based on their relationships with other objects. Effectively, we compute a measure that says "two objects are similar if they are related to similar objects:" This general similarity measure, called SimRank, is based on a simple and intuitive graph-theoretic model. For a given domain, SimRank can be combined with other domain-specific similarity measures. We suggest techniques for efficient computation of SimRank scores, and provide experimental results on two application domains showing the computational feasibility and effectiveness of our approach.},
	urldate = {2019-08-27},
	booktitle = {Proceedings of the {Eighth} {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {ACM},
	author = {Jeh, Glen and Widom, Jennifer},
	year = {2002},
	note = {event-place: Edmonton, Alberta, Canada},
	pages = {538--543},
	file = {ACM Full Text PDF:/Users/yl/Zotero/storage/QQKF64HW/Jeh and Widom - 2002 - SimRank A Measure of Structural-context Similarit.pdf:application/pdf}
}

@article{wang_knowledge_2014,
	title = {Knowledge {Graph} {Embedding} by {Translating} on {Hyperplanes}},
	abstract = {We deal with embedding a large scale knowledge graph composed of entities and relations into a continuous vector space. TransE is a promising method proposed recently, which is very efﬁcient while achieving state-of-the-art predictive performance. We discuss some mapping properties of relations which should be considered in embedding, such as reﬂexive, one-to-many, many-to-one, and many-to-many. We note that TransE does not do well in dealing with these properties. Some complex models are capable of preserving these mapping properties but sacriﬁce efﬁciency in the process. To make a good trade-off between model capacity and efﬁciency, in this paper we propose TransH which models a relation as a hyperplane together with a translation operation on it. In this way, we can well preserve the above mapping properties of relations with almost the same model complexity of TransE. Additionally, as a practical knowledge graph is often far from completed, how to construct negative examples to reduce false negative labels in training is very important. Utilizing the one-to-many/many-to-one mapping property of a relation, we propose a simple trick to reduce the possibility of false negative labeling. We conduct extensive experiments on link prediction, triplet classiﬁcation and fact extraction on benchmark datasets like WordNet and Freebase. Experiments show TransH delivers signiﬁcant improvements over TransE on predictive accuracy with comparable capability to scale up.},
	language = {en},
	journal = {AAAI 14},
	author = {Wang, Zhen and Zhang, Jianwen and Feng, Jianlin and Chen, Zheng},
	year = {2014},
	pages = {8},
	file = {Wang et al. - Knowledge Graph Embedding by Translating on Hyperp.pdf:/Users/yl/Zotero/storage/CKBSRRHA/Wang et al. - Knowledge Graph Embedding by Translating on Hyperp.pdf:application/pdf}
}

@article{lin_learning_2015,
	title = {Learning {Entity} and {Relation} {Embeddings} for {Knowledge} {Graph} {Completion}},
	abstract = {Knowledge graph completion aims to perform link prediction between entities. In this paper, we consider the approach of knowledge graph embeddings. Recently, models such as TransE and TransH build entity and relation embeddings by regarding a relation as translation from head entity to tail entity. We note that these models simply put both entities and relations within the same semantic space. In fact, an entity may have multiple aspects and various relations may focus on different aspects of entities, which makes a common space insufﬁcient for modeling. In this paper, we propose TransR to build entity and relation embeddings in separate entity space and relation spaces. Afterwards, we learn embeddings by ﬁrst projecting entities from entity space to corresponding relation space and then building translations between projected entities. In experiments, we evaluate our models on three tasks including link prediction, triple classiﬁcation and relational fact extraction. Experimental results show significant and consistent improvements compared to stateof-the-art baselines including TransE and TransH. The source code of this paper can be obtained from https: //github.com/mrlyk423/relation extraction.},
	language = {en},
	journal = {AAAI 15},
	author = {Lin, Yankai and Liu, Zhiyuan and Sun, Maosong and Liu, Yang and Zhu, Xuan},
	year = {2015},
	pages = {7},
	file = {Lin et al. - Learning Entity and Relation Embeddings for Knowle.pdf:/Users/yl/Zotero/storage/P72STUGV/Lin et al. - Learning Entity and Relation Embeddings for Knowle.pdf:application/pdf}
}

@incollection{socher_reasoning_2013,
	title = {Reasoning {With} {Neural} {Tensor} {Networks} for {Knowledge} {Base} {Completion}},
	url = {http://papers.nips.cc/paper/5028-reasoning-with-neural-tensor-networks-for-knowledge-base-completion.pdf},
	urldate = {2019-08-28},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 26},
	publisher = {Curran Associates, Inc.},
	author = {Socher, Richard and Chen, Danqi and Manning, Christopher D and Ng, Andrew},
	editor = {Burges, C. J. C. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
	year = {2013},
	pages = {926--934},
	file = {NIPS Full Text PDF:/Users/yl/Zotero/storage/6RMMWUZ2/Socher et al. - 2013 - Reasoning With Neural Tensor Networks for Knowledg.pdf:application/pdf;NIPS Snapshot:/Users/yl/Zotero/storage/LCP7NSD5/5028-reasoning-with-neural-tensor-networks-for-knowledge-base-completion.html:text/html}
}

@article{kipf_variational_2016,
	title = {Variational {Graph} {Auto}-{Encoders}},
	url = {http://arxiv.org/abs/1611.07308},
	abstract = {We introduce the variational graph auto-encoder (VGAE), a framework for unsupervised learning on graph-structured data based on the variational auto-encoder (VAE). This model makes use of latent variables and is capable of learning interpretable latent representations for undirected graphs. We demonstrate this model using a graph convolutional network (GCN) encoder and a simple inner product decoder. Our model achieves competitive results on a link prediction task in citation networks. In contrast to most existing models for unsupervised learning on graph-structured data and link prediction, our model can naturally incorporate node features, which significantly improves predictive performance on a number of benchmark datasets.},
	urldate = {2019-08-30},
	journal = {arXiv:1611.07308 [cs, stat]},
	author = {Kipf, Thomas N. and Welling, Max},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.07308},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1611.07308 PDF:/Users/yl/Zotero/storage/CCHZV3SF/Kipf and Welling - 2016 - Variational Graph Auto-Encoders.pdf:application/pdf;arXiv.org Snapshot:/Users/yl/Zotero/storage/9HKFSKTY/1611.html:text/html}
}

@inproceedings{toutanova_observed_2015,
	title = {Observed versus latent features for knowledge base and text inference},
	url = {https://www.aclweb.org/anthology/W15-4007/},
	doi = {10.18653/v1/W15-4007},
	language = {en-us},
	urldate = {2019-08-30},
	booktitle = {{ACL}},
	author = {Toutanova, Kristina and Chen, Danqi},
	month = jul,
	year = {2015},
	pages = {57--66},
	file = {Full Text PDF:/Users/yl/Zotero/storage/S75FZPNV/Toutanova and Chen - 2015 - Observed versus latent features for knowledge base.pdf:application/pdf;Snapshot:/Users/yl/Zotero/storage/52PRRRA9/W15-4007.html:text/html}
}

@article{dettmers_convolutional_2017,
	title = {Convolutional 2D {Knowledge} {Graph} {Embeddings}},
	url = {http://arxiv.org/abs/1707.01476},
	abstract = {Link prediction for knowledge graphs is the task of predicting missing relationships between entities. Previous work on link prediction has focused on shallow, fast models which can scale to large knowledge graphs. However, these models learn less expressive features than deep, multi-layer models -- which potentially limits performance. In this work, we introduce ConvE, a multi-layer convolutional network model for link prediction, and report state-of-the-art results for several established datasets. We also show that the model is highly parameter efficient, yielding the same performance as DistMult and R-GCN with 8x and 17x fewer parameters. Analysis of our model suggests that it is particularly effective at modelling nodes with high indegree -- which are common in highly-connected, complex knowledge graphs such as Freebase and YAGO3. In addition, it has been noted that the WN18 and FB15k datasets suffer from test set leakage, due to inverse relations from the training set being present in the test set -- however, the extent of this issue has so far not been quantified. We find this problem to be severe: a simple rule-based model can achieve state-of-the-art results on both WN18 and FB15k. To ensure that models are evaluated on datasets where simply exploiting inverse relations cannot yield competitive results, we investigate and validate several commonly used datasets -- deriving robust variants where necessary. We then perform experiments on these robust datasets for our own and several previously proposed models and find that ConvE achieves state-of-the-art Mean Reciprocal Rank across most datasets.},
	urldate = {2019-08-30},
	journal = {arXiv:1707.01476 [cs]},
	author = {Dettmers, Tim and Minervini, Pasquale and Stenetorp, Pontus and Riedel, Sebastian},
	month = jul,
	year = {2017},
	note = {arXiv: 1707.01476},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv\:1707.01476 PDF:/Users/yl/Zotero/storage/7A9MNH6G/Dettmers et al. - 2017 - Convolutional 2D Knowledge Graph Embeddings.pdf:application/pdf;arXiv.org Snapshot:/Users/yl/Zotero/storage/L6EPGI5N/1707.html:text/html}
}

@inproceedings{bauer_commonsense_2018,
	address = {Brussels, Belgium},
	title = {Commonsense for {Generative} {Multi}-{Hop} {Question} {Answering} {Tasks}},
	url = {https://www.aclweb.org/anthology/D18-1454},
	doi = {10.18653/v1/D18-1454},
	abstract = {Reading comprehension QA tasks have seen a recent surge in popularity, yet most works have focused on fact-finding extractive QA. We instead focus on a more challenging multi-hop generative task (NarrativeQA), which requires the model to reason, gather, and synthesize disjoint pieces of information within the context to generate an answer. This type of multi-step reasoning also often requires understanding implicit relations, which humans resolve via external, background commonsense knowledge. We first present a strong generative baseline that uses a multi-attention mechanism to perform multiple hops of reasoning and a pointer-generator decoder to synthesize the answer. This model performs substantially better than previous generative models, and is competitive with current state-of-the-art span prediction models. We next introduce a novel system for selecting grounded multi-hop relational commonsense information from ConceptNet via a pointwise mutual information and term-frequency based scoring function. Finally, we effectively use this extracted commonsense information to fill in gaps of reasoning between context hops, using a selectively-gated attention mechanism. This boosts the model's performance significantly (also verified via human evaluation), establishing a new state-of-the-art for the task. We also show that our background knowledge enhancements are generalizable and improve performance on QAngaroo-WikiHop, another multi-hop reasoning dataset.},
	urldate = {2019-08-30},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Bauer, Lisa and Wang, Yicheng and Bansal, Mohit},
	month = oct,
	year = {2018},
	pages = {4220--4230},
	file = {Full Text PDF:/Users/yl/Zotero/storage/87DBTNZN/Bauer et al. - 2018 - Commonsense for Generative Multi-Hop Question Answ.pdf:application/pdf}
}

@inproceedings{zhuang_token-level_2019,
	address = {Florence, Italy},
	title = {Token-level {Dynamic} {Self}-{Attention} {Network} for {Multi}-{Passage} {Reading} {Comprehension}},
	url = {https://www.aclweb.org/anthology/P19-1218},
	abstract = {Multi-passage reading comprehension requires the ability to combine cross-passage information and reason over multiple passages to infer the answer. In this paper, we introduce the Dynamic Self-attention Network (DynSAN) for multi-passage reading comprehension task, which processes cross-passage information at token-level and meanwhile avoids substantial computational costs. The core module of the dynamic self-attention is a proposed gated token selection mechanism, which dynamically selects important tokens from a sequence. These chosen tokens will attend to each other via a self-attention mechanism to model long-range dependencies. Besides, convolutional layers are combined with the dynamic self-attention to enhance the model's capacity of extracting local semantic. The experimental results show that the proposed DynSAN achieves new state-of-the-art performance on the SearchQA, Quasar-T and WikiHop datasets. Further ablation study also validates the effectiveness of our model components.},
	urldate = {2019-08-30},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Zhuang, Yimeng and Wang, Huadong},
	month = jul,
	year = {2019},
	pages = {2252--2262},
	file = {Full Text PDF:/Users/yl/Zotero/storage/WFZLYDBS/Zhuang and Wang - 2019 - Token-level Dynamic Self-Attention Network for Mul.pdf:application/pdf}
}

@article{seo_bidirectional_2016,
	title = {Bidirectional {Attention} {Flow} for {Machine} {Comprehension}},
	volume = {abs/1611.01603},
	abstract = {Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.},
	journal = {ArXiv},
	author = {Seo, Min Joon and Kembhavi, Aniruddha and Farhadi, Ali and Hajishirzi, Hannaneh},
	year = {2016},
	keywords = {Bi-directional text, Interaction, List comprehension, Stanford University centers and institutes, Temporal logic},
	file = {Full Text PDF:/Users/yl/Zotero/storage/I49JZ9TG/Seo et al. - 2016 - Bidirectional Attention Flow for Machine Comprehen.pdf:application/pdf}
}

@article{zhong_coarse-grain_2019,
	title = {Coarse-grain {Fine}-grain {Coattention} {Network} for {Multi}-evidence {Question} {Answering}},
	url = {http://arxiv.org/abs/1901.00603},
	abstract = {End-to-end neural models have made significant progress in question answering, however recent studies show that these models implicitly assume that the answer and evidence appear close together in a single document. In this work, we propose the Coarse-grain Fine-grain Coattention Network (CFC), a new question answering model that combines information from evidence across multiple documents. The CFC consists of a coarse-grain module that interprets documents with respect to the query then finds a relevant answer, and a fine-grain module which scores each candidate answer by comparing its occurrences across all of the documents with the query. We design these modules using hierarchies of coattention and self-attention, which learn to emphasize different parts of the input. On the Qangaroo WikiHop multi-evidence question answering task, the CFC obtains a new state-of-the-art result of 70.6\% on the blind test set, outperforming the previous best by 3\% accuracy despite not using pretrained contextual encoders.},
	urldate = {2019-08-30},
	journal = {arXiv:1901.00603 [cs]},
	author = {Zhong, Victor and Xiong, Caiming and Keskar, Nitish Shirish and Socher, Richard},
	month = jan,
	year = {2019},
	note = {arXiv: 1901.00603},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv\:1901.00603 PDF:/Users/yl/Zotero/storage/X55GJM45/Zhong et al. - 2019 - Coarse-grain Fine-grain Coattention Network for Mu.pdf:application/pdf;arXiv.org Snapshot:/Users/yl/Zotero/storage/E25CVYMM/1901.html:text/html}
}

@article{raison_weaver:_2018,
	title = {Weaver: {Deep} {Co}-{Encoding} of {Questions} and {Documents} for {Machine} {Reading}},
	shorttitle = {Weaver},
	url = {http://arxiv.org/abs/1804.10490},
	abstract = {This paper aims at improving how machines can answer questions directly from text, with the focus of having models that can answer correctly multiple types of questions and from various types of texts, documents or even from large collections of them. To that end, we introduce the Weaver model that uses a new way to relate a question to a textual context by weaving layers of recurrent networks, with the goal of making as few assumptions as possible as to how the information from both question and context should be combined to form the answer. We show empirically on six datasets that Weaver performs well in multiple conditions. For instance, it produces solid results on the very popular SQuAD dataset (Rajpurkar et al., 2016), solves almost all bAbI tasks (Weston et al., 2015) and greatly outperforms state-of-the-art methods for open domain question answering from text (Chen et al., 2017).},
	urldate = {2019-08-30},
	journal = {arXiv:1804.10490 [cs]},
	author = {Raison, Martin and Mazaré, Pierre-Emmanuel and Das, Rajarshi and Bordes, Antoine},
	month = apr,
	year = {2018},
	note = {arXiv: 1804.10490},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv\:1804.10490 PDF:/Users/yl/Zotero/storage/E7U9URCP/Raison et al. - 2018 - Weaver Deep Co-Encoding of Questions and Document.pdf:application/pdf;arXiv.org Snapshot:/Users/yl/Zotero/storage/8VHDXQNA/1804.html:text/html}
}

@inproceedings{dhingra_neural_2018,
	address = {New Orleans, Louisiana},
	title = {Neural {Models} for {Reasoning} over {Multiple} {Mentions} {Using} {Coreference}},
	url = {https://www.aclweb.org/anthology/N18-2007},
	doi = {10.18653/v1/N18-2007},
	abstract = {Many problems in NLP require aggregating information from multiple mentions of the same entity which may be far apart in the text. Existing Recurrent Neural Network (RNN) layers are biased towards short-term dependencies and hence not suited to such tasks. We present a recurrent layer which is instead biased towards coreferent dependencies. The layer uses coreference annotations extracted from an external system to connect entity mentions belonging to the same cluster. Incorporating this layer into a state-of-the-art reading comprehension model improves performance on three datasets – Wikihop, LAMBADA and the bAbi AI tasks – with large gains when training data is scarce.},
	urldate = {2019-08-30},
	booktitle = {Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 2 ({Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Dhingra, Bhuwan and Jin, Qiao and Yang, Zhilin and Cohen, William and Salakhutdinov, Ruslan},
	month = jun,
	year = {2018},
	pages = {42--48},
	file = {Full Text PDF:/Users/yl/Zotero/storage/VAF94ZQS/Dhingra et al. - 2018 - Neural Models for Reasoning over Multiple Mentions.pdf:application/pdf}
}

@article{cao_bag:_2019,
	title = {{BAG}: {Bi}-directional {Attention} {Entity} {Graph} {Convolutional} {Network} for {Multi}-hop {Reasoning} {Question} {Answering}},
	shorttitle = {{BAG}},
	url = {http://arxiv.org/abs/1904.04969},
	abstract = {Multi-hop reasoning question answering requires deep comprehension of relationships between various documents and queries. We propose a Bi-directional Attention Entity Graph Convolutional Network (BAG), leveraging relationships between nodes in an entity graph and attention information between a query and the entity graph, to solve this task. Graph convolutional networks are used to obtain a relation-aware representation of nodes for entity graphs built from documents with multi-level features. Bidirectional attention is then applied on graphs and queries to generate a query-aware nodes representation, which will be used for the final prediction. Experimental evaluation shows BAG achieves state-of-the-art accuracy performance on the QAngaroo WIKIHOP dataset.},
	urldate = {2019-08-30},
	journal = {arXiv:1904.04969 [cs]},
	author = {Cao, Yu and Fang, Meng and Tao, Dacheng},
	month = apr,
	year = {2019},
	note = {arXiv: 1904.04969},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv\:1904.04969 PDF:/Users/yl/Zotero/storage/G33L92FG/Cao et al. - 2019 - BAG Bi-directional Attention Entity Graph Convolu.pdf:application/pdf;arXiv.org Snapshot:/Users/yl/Zotero/storage/JMAITM2M/1904.html:text/html}
}

@inproceedings{tu_multi-hop_2019,
	address = {Florence, Italy},
	title = {Multi-hop {Reading} {Comprehension} across {Multiple} {Documents} by {Reasoning} over {Heterogeneous} {Graphs}},
	url = {https://www.aclweb.org/anthology/P19-1260},
	abstract = {Multi-hop reading comprehension (RC) across documents poses new challenge over single-document RC because it requires reasoning over multiple documents to reach the final answer. In this paper, we propose a new model to tackle the multi-hop RC problem. We introduce a heterogeneous graph with different types of nodes and edges, which is named as Heterogeneous Document-Entity (HDE) graph. The advantage of HDE graph is that it contains different granularity levels of information including candidates, documents and entities in specific document contexts. Our proposed model can do reasoning over the HDE graph with nodes representation initialized with co-attention and self-attention based context encoders. We employ Graph Neural Networks (GNN) based message passing algorithms to accumulate evidences on the proposed HDE graph. Evaluated on the blind test set of the Qangaroo WikiHop data set, our HDE graph based single model delivers competitive result, and the ensemble model achieves the state-of-the-art performance.},
	urldate = {2019-08-30},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Tu, Ming and Wang, Guangtao and Huang, Jing and Tang, Yun and He, Xiaodong and Zhou, Bowen},
	month = jul,
	year = {2019},
	pages = {2704--2713},
	file = {Full Text PDF:/Users/yl/Zotero/storage/3CASP23A/Tu et al. - 2019 - Multi-hop Reading Comprehension across Multiple Do.pdf:application/pdf}
}

@article{hill_goldilocks_2015,
	title = {The {Goldilocks} {Principle}: {Reading} {Children}'s {Books} with {Explicit} {Memory} {Representations}},
	shorttitle = {The {Goldilocks} {Principle}},
	url = {http://arxiv.org/abs/1511.02301},
	abstract = {We introduce a new test of how well language models capture meaning in children's books. Unlike standard language modelling benchmarks, it distinguishes the task of predicting syntactic function words from that of predicting lower-frequency words, which carry greater semantic content. We compare a range of state-of-the-art models, each with a different way of encoding what has been previously read. We show that models which store explicit representations of long-term contexts outperform state-of-the-art neural language models at predicting semantic content words, although this advantage is not observed for syntactic function words. Interestingly, we find that the amount of text encoded in a single memory representation is highly influential to the performance: there is a sweet-spot, not too big and not too small, between single words and full sentences that allows the most meaningful information in a text to be effectively retained and recalled. Further, the attention over such window-based memories can be trained effectively through self-supervision. We then assess the generality of this principle by applying it to the CNN QA benchmark, which involves identifying named entities in paraphrased summaries of news articles, and achieve state-of-the-art performance.},
	urldate = {2019-08-30},
	journal = {arXiv:1511.02301 [cs]},
	author = {Hill, Felix and Bordes, Antoine and Chopra, Sumit and Weston, Jason},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.02301},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv\:1511.02301 PDF:/Users/yl/Zotero/storage/A5QTAJSM/Hill et al. - 2015 - The Goldilocks Principle Reading Children's Books.pdf:application/pdf;arXiv.org Snapshot:/Users/yl/Zotero/storage/BKC8ZFKC/1511.html:text/html}
}

@inproceedings{sun_improving_2019,
	address = {Minneapolis, Minnesota},
	title = {Improving {Machine} {Reading} {Comprehension} with {General} {Reading} {Strategies}},
	url = {https://www.aclweb.org/anthology/N19-1270},
	doi = {10.18653/v1/N19-1270},
	abstract = {Reading strategies have been shown to improve comprehension levels, especially for readers lacking adequate prior knowledge. Just as the process of knowledge accumulation is time-consuming for human readers, it is resource-demanding to impart rich general domain knowledge into a deep language model via pre-training. Inspired by reading strategies identified in cognitive science, and given limited computational resources - just a pre-trained model and a fixed number of training instances - we propose three general strategies aimed to improve non-extractive machine reading comprehension (MRC): (i) BACK AND FORTH READING that considers both the original and reverse order of an input sequence, (ii) HIGHLIGHTING, which adds a trainable embedding to the text embedding of tokens that are relevant to the question and candidate answers, and (iii) SELF-ASSESSMENT that generates practice questions and candidate answers directly from the text in an unsupervised manner. By fine-tuning a pre-trained language model (Radford et al., 2018) with our proposed strategies on the largest general domain multiple-choice MRC dataset RACE, we obtain a 5.8\% absolute increase in accuracy over the previous best result achieved by the same pre-trained model fine-tuned on RACE without the use of strategies. We further fine-tune the resulting model on a target MRC task, leading to an absolute improvement of 6.2\% in average accuracy over previous state-of-the-art approaches on six representative non-extractive MRC datasets from different domains (i.e., ARC, OpenBookQA, MCTest, SemEval-2018 Task 11, ROCStories, and MultiRC). These results demonstrate the effectiveness of our proposed strategies and the versatility and general applicability of our fine-tuned models that incorporate these strategies. Core code is available at https://github.com/nlpdata/strategy/.},
	urldate = {2019-08-30},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Sun, Kai and Yu, Dian and Yu, Dong and Cardie, Claire},
	month = jun,
	year = {2019},
	pages = {2633--2643},
	file = {Full Text PDF:/Users/yl/Zotero/storage/P6LWLGGG/Sun et al. - 2019 - Improving Machine Reading Comprehension with Gener.pdf:application/pdf}
}

@article{wu_global--local_2019,
	title = {Global-to-local {Memory} {Pointer} {Networks} for {Task}-{Oriented} {Dialogue}},
	url = {http://arxiv.org/abs/1901.04713},
	abstract = {End-to-end task-oriented dialogue is challenging since knowledge bases are usually large, dynamic and hard to incorporate into a learning framework. We propose the global-to-local memory pointer (GLMP) networks to address this issue. In our model, a global memory encoder and a local memory decoder are proposed to share external knowledge. The encoder encodes dialogue history, modifies global contextual representation, and generates a global memory pointer. The decoder first generates a sketch response with unfilled slots. Next, it passes the global memory pointer to filter the external knowledge for relevant information, then instantiates the slots via the local memory pointers. We empirically show that our model can improve copy accuracy and mitigate the common out-of-vocabulary problem. As a result, GLMP is able to improve over the previous state-of-the-art models in both simulated bAbI Dialogue dataset and human-human Stanford Multi-domain Dialogue dataset on automatic and human evaluation.},
	urldate = {2019-08-30},
	journal = {arXiv:1901.04713 [cs]},
	author = {Wu, Chien-Sheng and Socher, Richard and Xiong, Caiming},
	month = jan,
	year = {2019},
	note = {arXiv: 1901.04713},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv\:1901.04713 PDF:/Users/yl/Zotero/storage/GZE8SDNY/Wu et al. - 2019 - Global-to-local Memory Pointer Networks for Task-O.pdf:application/pdf;arXiv.org Snapshot:/Users/yl/Zotero/storage/6L2NSANB/1901.html:text/html}
}

@inproceedings{chen_reading_2017,
	title = {Reading {Wikipedia} to {Answer} {Open}-{Domain} {Questions}},
	url = {https://www.aclweb.org/anthology/P17-1171/},
	doi = {10.18653/v1/P17-1171},
	language = {en-us},
	urldate = {2019-08-30},
	booktitle = {{ACL}},
	author = {Chen, Danqi and Fisch, Adam and Weston, Jason and Bordes, Antoine},
	month = jul,
	year = {2017},
	pages = {1870--1879},
	file = {Full Text PDF:/Users/yl/Zotero/storage/36AM4IF8/Chen et al. - 2017 - Reading Wikipedia to Answer Open-Domain Questions.pdf:application/pdf;Snapshot:/Users/yl/Zotero/storage/785SIB3D/P17-1171.html:text/html}
}

@article{dhingra_quasar:_2017,
	title = {Quasar: {Datasets} for {Question} {Answering} by {Search} and {Reading}},
	volume = {abs/1707.03904},
	shorttitle = {Quasar},
	abstract = {We present two new large-scale datasets aimed at evaluating systems designed to comprehend a natural language query and extract its answer from a large corpus of text. The Quasar-S dataset consists of 37000 cloze-style (fill-in-the-gap) queries constructed from definitions of software entity tags on the popular website Stack Overflow. The posts and comments on the website serve as the background corpus for answering the cloze questions. The Quasar-T dataset consists of 43000 open-domain trivia questions and their answers obtained from various internet sources. ClueWeb09 serves as the background corpus for extracting these answers. We pose these datasets as a challenge for two related subtasks of factoid Question Answering: (1) searching for relevant pieces of text that include the correct answer to a query, and (2) reading the retrieved text to answer the query. We also describe a retrieval system for extracting relevant sentences and documents from the corpus given a query, and include these in the release for researchers wishing to only focus on (2). We evaluate several baselines on both datasets, ranging from simple heuristics to powerful neural models, and show that these lag behind human performance by 16.4\% and 32.1\% for Quasar-S and -T respectively. The datasets are available at this https URL .},
	journal = {ArXiv},
	author = {Dhingra, Bhuwan and Mazaitis, Kathryn and Cohen, William W.},
	year = {2017},
	note = {arXiv: 1707.03904},
	keywords = {Baseline (configuration management), Heuristic (computer science), HTTPS, Human reliability, Natural language user interface, Question answering, Stack Overflow, Text corpus},
	file = {Full Text PDF:/Users/yl/Zotero/storage/WNI7TKFU/Dhingra et al. - 2017 - Quasar Datasets for Question Answering by Search .pdf:application/pdf}
}

@inproceedings{clark_simple_2018,
	address = {Melbourne, Australia},
	title = {Simple and {Effective} {Multi}-{Paragraph} {Reading} {Comprehension}},
	url = {https://www.aclweb.org/anthology/P18-1078},
	doi = {10.18653/v1/P18-1078},
	abstract = {We introduce a method of adapting neural paragraph-level question answering models to the case where entire documents are given as input. Most current question answering models cannot scale to document or multi-document input, and naively applying these models to each paragraph independently often results in them being distracted by irrelevant text. We show that it is possible to significantly improve performance by using a modified training scheme that teaches the model to ignore non-answer containing paragraphs. Our method involves sampling multiple paragraphs from each document, and using an objective function that requires the model to produce globally correct output. We additionally identify and improve upon a number of other design decisions that arise when working with document-level data. Experiments on TriviaQA and SQuAD shows our method advances the state of the art, including a 10 point gain on TriviaQA.},
	urldate = {2019-08-30},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Clark, Christopher and Gardner, Matt},
	month = jul,
	year = {2018},
	pages = {845--855},
	file = {Full Text PDF:/Users/yl/Zotero/storage/D6IIH3GN/Clark and Gardner - 2018 - Simple and Effective Multi-Paragraph Reading Compr.pdf:application/pdf}
}

@article{wang_r$^3$:_2017,
	title = {R\${\textasciicircum}3\$: {Reinforced} {Reader}-{Ranker} for {Open}-{Domain} {Question} {Answering}},
	shorttitle = {R\${\textasciicircum}3\$},
	url = {http://arxiv.org/abs/1709.00023},
	abstract = {In recent years researchers have achieved considerable success applying neural network methods to question answering (QA). These approaches have achieved state of the art results in simplified closed-domain settings such as the SQuAD (Rajpurkar et al., 2016) dataset, which provides a pre-selected passage, from which the answer to a given question may be extracted. More recently, researchers have begun to tackle open-domain QA, in which the model is given a question and access to a large corpus (e.g., wikipedia) instead of a pre-selected passage (Chen et al., 2017a). This setting is more complex as it requires large-scale search for relevant passages by an information retrieval component, combined with a reading comprehension model that "reads" the passages to generate an answer to the question. Performance in this setting lags considerably behind closed-domain performance. In this paper, we present a novel open-domain QA system called Reinforced Ranker-Reader \$(R{\textasciicircum}3)\$, based on two algorithmic innovations. First, we propose a new pipeline for open-domain QA with a Ranker component, which learns to rank retrieved passages in terms of likelihood of generating the ground-truth answer to a given question. Second, we propose a novel method that jointly trains the Ranker along with an answer-generation Reader model, based on reinforcement learning. We report extensive experimental results showing that our method significantly improves on the state of the art for multiple open-domain QA datasets.},
	urldate = {2019-08-30},
	journal = {arXiv:1709.00023 [cs]},
	author = {Wang, Shuohang and Yu, Mo and Guo, Xiaoxiao and Wang, Zhiguo and Klinger, Tim and Zhang, Wei and Chang, Shiyu and Tesauro, Gerald and Zhou, Bowen and Jiang, Jing},
	month = aug,
	year = {2017},
	note = {arXiv: 1709.00023},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv\:1709.00023 PDF:/Users/yl/Zotero/storage/ZLMCSKUT/Wang et al. - 2017 - R\$^3\$ Reinforced Reader-Ranker for Open-Domain Qu.pdf:application/pdf;arXiv.org Snapshot:/Users/yl/Zotero/storage/QY47PQ5A/1709.html:text/html}
}

@article{dunn_searchqa:_2017,
	title = {{SearchQA}: {A} {New} {Q}\&{A} {Dataset} {Augmented} with {Context} from a {Search} {Engine}},
	shorttitle = {{SearchQA}},
	url = {http://arxiv.org/abs/1704.05179},
	abstract = {We publicly release a new large-scale dataset, called SearchQA, for machine comprehension, or question-answering. Unlike recently released datasets, such as DeepMind CNN/DailyMail and SQuAD, the proposed SearchQA was constructed to reflect a full pipeline of general question-answering. That is, we start not from an existing article and generate a question-answer pair, but start from an existing question-answer pair, crawled from J! Archive, and augment it with text snippets retrieved by Google. Following this approach, we built SearchQA, which consists of more than 140k question-answer pairs with each pair having 49.6 snippets on average. Each question-answer-context tuple of the SearchQA comes with additional meta-data such as the snippet's URL, which we believe will be valuable resources for future research. We conduct human evaluation as well as test two baseline methods, one simple word selection and the other deep learning based, on the SearchQA. We show that there is a meaningful gap between the human and machine performances. This suggests that the proposed dataset could well serve as a benchmark for question-answering.},
	urldate = {2019-08-30},
	journal = {arXiv:1704.05179 [cs]},
	author = {Dunn, Matthew and Sagun, Levent and Higgins, Mike and Guney, V. Ugur and Cirik, Volkan and Cho, Kyunghyun},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.05179},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv\:1704.05179 PDF:/Users/yl/Zotero/storage/2Q4MG4NG/Dunn et al. - 2017 - SearchQA A New Q&A Dataset Augmented with Context.pdf:application/pdf;arXiv.org Snapshot:/Users/yl/Zotero/storage/JP23QN62/1704.html:text/html}
}

@inproceedings{mihaylov_can_2018,
	address = {Brussels, Belgium},
	title = {Can a {Suit} of {Armor} {Conduct} {Electricity}? {A} {New} {Dataset} for {Open} {Book} {Question} {Answering}},
	shorttitle = {Can a {Suit} of {Armor} {Conduct} {Electricity}?},
	url = {https://www.aclweb.org/anthology/D18-1260},
	doi = {10.18653/v1/D18-1260},
	abstract = {We present a new kind of question answering dataset, OpenBookQA, modeled after open book exams for assessing human understanding of a subject. The open book that comes with our questions is a set of 1326 elementary level science facts. Roughly 6000 questions probe an understanding of these facts and their application to novel situations. This requires combining an open book fact (e.g., metals conduct electricity) with broad common knowledge (e.g., a suit of armor is made of metal) obtained from other sources. While existing QA datasets over documents or knowledge bases, being generally self-contained, focus on linguistic understanding, OpenBookQA probes a deeper understanding of both the topic—in the context of common knowledge—and the language it is expressed in. Human performance on OpenBookQA is close to 92\%, but many state-of-the-art pre-trained QA methods perform surprisingly poorly, worse than several simple neural baselines we develop. Our oracle experiments designed to circumvent the knowledge retrieval bottleneck demonstrate the value of both the open book and additional facts. We leave it as a challenge to solve the retrieval problem in this multi-hop setting and to close the large gap to human performance.},
	urldate = {2019-08-30},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Mihaylov, Todor and Clark, Peter and Khot, Tushar and Sabharwal, Ashish},
	month = oct,
	year = {2018},
	pages = {2381--2391},
	file = {Full Text PDF:/Users/yl/Zotero/storage/8NKM9S9C/Mihaylov et al. - 2018 - Can a Suit of Armor Conduct Electricity A New Dat.pdf:application/pdf}
}

@article{weston_towards_2015,
	title = {Towards {AI}-{Complete} {Question} {Answering}: {A} {Set} of {Prerequisite} {Toy} {Tasks}},
	shorttitle = {Towards {AI}-{Complete} {Question} {Answering}},
	url = {http://arxiv.org/abs/1502.05698},
	abstract = {One long-term goal of machine learning research is to produce methods that are applicable to reasoning and natural language, in particular building an intelligent dialogue agent. To measure progress towards that goal, we argue for the usefulness of a set of proxy tasks that evaluate reading comprehension via question answering. Our tasks measure understanding in several ways: whether a system is able to answer questions via chaining facts, simple induction, deduction and many more. The tasks are designed to be prerequisites for any system that aims to be capable of conversing with a human. We believe many existing learning systems can currently not solve them, and hence our aim is to classify these tasks into skill sets, so that researchers can identify (and then rectify) the failings of their systems. We also extend and improve the recently introduced Memory Networks model, and show it is able to solve some, but not all, of the tasks.},
	urldate = {2019-08-30},
	journal = {arXiv:1502.05698 [cs, stat]},
	author = {Weston, Jason and Bordes, Antoine and Chopra, Sumit and Rush, Alexander M. and van Merriënboer, Bart and Joulin, Armand and Mikolov, Tomas},
	month = feb,
	year = {2015},
	note = {arXiv: 1502.05698},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Statistics - Machine Learning},
	file = {arXiv\:1502.05698 PDF:/Users/yl/Zotero/storage/EMYEDQTY/Weston et al. - 2015 - Towards AI-Complete Question Answering A Set of P.pdf:application/pdf;arXiv.org Snapshot:/Users/yl/Zotero/storage/IXL7CYU6/1502.html:text/html}
}

@article{bahdanau_neural_2014,
	title = {Neural {Machine} {Translation} by {Jointly} {Learning} to {Align} and {Translate}},
	url = {http://arxiv.org/abs/1409.0473},
	abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
	urldate = {2019-09-02},
	journal = {arXiv:1409.0473 [cs, stat]},
	author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	month = sep,
	year = {2014},
	note = {arXiv: 1409.0473},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv\:1409.0473 PDF:/Users/yl/Zotero/storage/VRVK5DPG/Bahdanau et al. - 2014 - Neural Machine Translation by Jointly Learning to .pdf:application/pdf;arXiv.org Snapshot:/Users/yl/Zotero/storage/549F9RNZ/1409.html:text/html}
}

@article{speer_conceptnet_2016,
	title = {{ConceptNet} 5.5: {An} {Open} {Multilingual} {Graph} of {General} {Knowledge}},
	shorttitle = {{ConceptNet} 5.5},
	url = {http://arxiv.org/abs/1612.03975},
	abstract = {Machine learning about language can be improved by supplying it with specific knowledge and sources of external information. We present here a new version of the linked open data resource ConceptNet that is particularly well suited to be used with modern NLP techniques such as word embeddings. ConceptNet is a knowledge graph that connects words and phrases of natural language with labeled edges. Its knowledge is collected from many sources that include expert-created resources, crowd-sourcing, and games with a purpose. It is designed to represent the general knowledge involved in understanding language, improving natural language applications by allowing the application to better understand the meanings behind the words people use. When ConceptNet is combined with word embeddings acquired from distributional semantics (such as word2vec), it provides applications with understanding that they would not acquire from distributional semantics alone, nor from narrower resources such as WordNet or DBPedia. We demonstrate this with state-of-the-art results on intrinsic evaluations of word relatedness that translate into improvements on applications of word vectors, including solving SAT-style analogies.},
	urldate = {2019-09-02},
	journal = {arXiv:1612.03975 [cs]},
	author = {Speer, Robyn and Chin, Joshua and Havasi, Catherine},
	month = dec,
	year = {2016},
	note = {arXiv: 1612.03975},
	keywords = {Computer Science - Computation and Language, I.2.7},
	file = {arXiv\:1612.03975 PDF:/Users/yl/Zotero/storage/9DLT6XW5/Speer et al. - 2016 - ConceptNet 5.5 An Open Multilingual Graph of Gene.pdf:application/pdf;arXiv.org Snapshot:/Users/yl/Zotero/storage/TXISRS7Z/1612.html:text/html}
}

@incollection{sukhbaatar_end--end_2015,
	title = {End-{To}-{End} {Memory} {Networks}},
	url = {http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf},
	urldate = {2019-09-02},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 28},
	publisher = {Curran Associates, Inc.},
	author = {Sukhbaatar, Sainbayar and szlam, arthur and Weston, Jason and Fergus, Rob},
	editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
	year = {2015},
	pages = {2440--2448},
	file = {NIPS Full Text PDF:/Users/yl/Zotero/storage/NWVLJEFZ/Sukhbaatar et al. - 2015 - End-To-End Memory Networks.pdf:application/pdf;NIPS Snapshot:/Users/yl/Zotero/storage/2DRFJICJ/5846-end-to-end-memory-networks.html:text/html}
}

@inproceedings{jain_question_2016,
	address = {San Diego, California},
	title = {Question {Answering} over {Knowledge} {Base} using {Factual} {Memory} {Networks}},
	url = {https://www.aclweb.org/anthology/N16-2016},
	doi = {10.18653/v1/N16-2016},
	urldate = {2019-09-02},
	booktitle = {Proceedings of the {NAACL} {Student} {Research} {Workshop}},
	publisher = {Association for Computational Linguistics},
	author = {Jain, Sarthak},
	month = jun,
	year = {2016},
	pages = {109--115},
	file = {Full Text PDF:/Users/yl/Zotero/storage/Z5FQA8LH/Jain - 2016 - Question Answering over Knowledge Base using Factu.pdf:application/pdf}
}

@article{neelakantan_neural_2015,
	title = {Neural {Programmer}: {Inducing} {Latent} {Programs} with {Gradient} {Descent}},
	shorttitle = {Neural {Programmer}},
	url = {http://arxiv.org/abs/1511.04834},
	abstract = {Deep neural networks have achieved impressive supervised classification performance in many tasks including image recognition, speech recognition, and sequence to sequence learning. However, this success has not been translated to applications like question answering that may involve complex arithmetic and logic reasoning. A major limitation of these models is in their inability to learn even simple arithmetic and logic operations. For example, it has been shown that neural networks fail to learn to add two binary numbers reliably. In this work, we propose Neural Programmer, an end-to-end differentiable neural network augmented with a small set of basic arithmetic and logic operations. Neural Programmer can call these augmented operations over several steps, thereby inducing compositional programs that are more complex than the built-in operations. The model learns from a weak supervision signal which is the result of execution of the correct program, hence it does not require expensive annotation of the correct program itself. The decisions of what operations to call, and what data segments to apply to are inferred by Neural Programmer. Such decisions, during training, are done in a differentiable fashion so that the entire network can be trained jointly by gradient descent. We find that training the model is difficult, but it can be greatly improved by adding random noise to the gradient. On a fairly complex synthetic table-comprehension dataset, traditional recurrent networks and attentional models perform poorly while Neural Programmer typically obtains nearly perfect accuracy.},
	urldate = {2019-09-02},
	journal = {arXiv:1511.04834 [cs, stat]},
	author = {Neelakantan, Arvind and Le, Quoc V. and Sutskever, Ilya},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.04834},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1511.04834 PDF:/Users/yl/Zotero/storage/RQULRKMG/Neelakantan et al. - 2015 - Neural Programmer Inducing Latent Programs with G.pdf:application/pdf;arXiv.org Snapshot:/Users/yl/Zotero/storage/IGGLNF9D/1511.html:text/html}
}

@inproceedings{yin_neural_2016,
	address = {San Diego, California},
	title = {Neural {Enquirer}: {Learning} to {Query} {Tables} in {Natural} {Language}},
	shorttitle = {Neural {Enquirer}},
	url = {https://www.aclweb.org/anthology/W16-0105},
	doi = {10.18653/v1/W16-0105},
	urldate = {2019-09-02},
	booktitle = {Proceedings of the {Workshop} on {Human}-{Computer} {Question} {Answering}},
	publisher = {Association for Computational Linguistics},
	author = {Yin, Pengcheng and Lu, Zhengdong and Li, Hang and Ben, Kao},
	month = jun,
	year = {2016},
	pages = {29--35},
	file = {Full Text PDF:/Users/yl/Zotero/storage/5GTQFCES/Yin et al. - 2016 - Neural Enquirer Learning to Query Tables in Natur.pdf:application/pdf}
}

@article{wang_evidence_2017,
	title = {Evidence {Aggregation} for {Answer} {Re}-{Ranking} in {Open}-{Domain} {Question} {Answering}},
	url = {http://arxiv.org/abs/1711.05116},
	abstract = {A popular recent approach to answering open-domain questions is to first search for question-related passages and then apply reading comprehension models to extract answers. Existing methods usually extract answers from single passages independently. But some questions require a combination of evidence from across different sources to answer correctly. In this paper, we propose two models which make use of multiple passages to generate their answers. Both use an answer-reranking approach which reorders the answer candidates generated by an existing state-of-the-art QA model. We propose two methods, namely, strength-based re-ranking and coverage-based re-ranking, to make use of the aggregated evidence from different passages to better determine the answer. Our models have achieved state-of-the-art results on three public open-domain QA datasets: Quasar-T, SearchQA and the open-domain version of TriviaQA, with about 8 percentage points of improvement over the former two datasets.},
	urldate = {2019-09-02},
	journal = {arXiv:1711.05116 [cs]},
	author = {Wang, Shuohang and Yu, Mo and Jiang, Jing and Zhang, Wei and Guo, Xiaoxiao and Chang, Shiyu and Wang, Zhiguo and Klinger, Tim and Tesauro, Gerald and Campbell, Murray},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.05116},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv\:1711.05116 PDF:/Users/yl/Zotero/storage/ZGR3GI7I/Wang et al. - 2017 - Evidence Aggregation for Answer Re-Ranking in Open.pdf:application/pdf;arXiv.org Snapshot:/Users/yl/Zotero/storage/RZ7LA83X/1711.html:text/html}
}

@inproceedings{wang_joint_2018,
	address = {Melbourne, Australia},
	title = {Joint {Training} of {Candidate} {Extraction} and {Answer} {Selection} for {Reading} {Comprehension}},
	url = {https://www.aclweb.org/anthology/P18-1159},
	doi = {10.18653/v1/P18-1159},
	abstract = {While sophisticated neural-based techniques have been developed in reading comprehension, most approaches model the answer in an independent manner, ignoring its relations with other answer candidates. This problem can be even worse in open-domain scenarios, where candidates from multiple passages should be combined to answer a single question. In this paper, we formulate reading comprehension as an extract-then-select two-stage procedure. We first extract answer candidates from passages, then select the final answer by combining information from all the candidates. Furthermore, we regard candidate extraction as a latent variable and train the two-stage process jointly with reinforcement learning. As a result, our approach has improved the state-of-the-art performance significantly on two challenging open-domain reading comprehension datasets. Further analysis demonstrates the effectiveness of our model components, especially the information fusion of all the candidates and the joint training of the extract-then-select procedure.},
	urldate = {2019-09-02},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Zhen and Liu, Jiachen and Xiao, Xinyan and Lyu, Yajuan and Wu, Tian},
	month = jul,
	year = {2018},
	pages = {1715--1724},
	file = {Full Text PDF:/Users/yl/Zotero/storage/DVX9UGB8/Wang et al. - 2018 - Joint Training of Candidate Extraction and Answer .pdf:application/pdf}
}

@inproceedings{lin_denoising_2018,
	address = {Melbourne, Australia},
	title = {Denoising {Distantly} {Supervised} {Open}-{Domain} {Question} {Answering}},
	url = {https://www.aclweb.org/anthology/P18-1161},
	doi = {10.18653/v1/P18-1161},
	abstract = {Distantly supervised open-domain question answering (DS-QA) aims to find answers in collections of unlabeled text. Existing DS-QA models usually retrieve related paragraphs from a large-scale corpus and apply reading comprehension technique to extract answers from the most relevant paragraph. They ignore the rich information contained in other paragraphs. Moreover, distant supervision data inevitably accompanies with the wrong labeling problem, and these noisy data will substantially degrade the performance of DS-QA. To address these issues, we propose a novel DS-QA model which employs a paragraph selector to filter out those noisy paragraphs and a paragraph reader to extract the correct answer from those denoised paragraphs. Experimental results on real-world datasets show that our model can capture useful information from noisy data and achieve significant improvements on DS-QA as compared to all baselines.},
	urldate = {2019-09-02},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Lin, Yankai and Ji, Haozhe and Liu, Zhiyuan and Sun, Maosong},
	month = jul,
	year = {2018},
	pages = {1736--1745},
	file = {Full Text PDF:/Users/yl/Zotero/storage/9I8SY6QD/Lin et al. - 2018 - Denoising Distantly Supervised Open-Domain Questio.pdf:application/pdf}
}

@inproceedings{pennington_glove:_2014,
	address = {Doha, Qatar},
	title = {Glove: {Global} {Vectors} for {Word} {Representation}},
	shorttitle = {Glove},
	url = {https://www.aclweb.org/anthology/D14-1162},
	doi = {10.3115/v1/D14-1162},
	urldate = {2019-09-02},
	booktitle = {Proceedings of the 2014 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
	month = oct,
	year = {2014},
	pages = {1532--1543},
	file = {Full Text PDF:/Users/yl/Zotero/storage/9VQ6NENI/Pennington et al. - 2014 - Glove Global Vectors for Word Representation.pdf:application/pdf}
}

@article{paszke_automatic_2017,
	title = {Automatic differentiation in {PyTorch}},
	url = {https://openreview.net/forum?id=BJJsrmfCZ},
	abstract = {In this article, we describe an automatic differentiation module of PyTorch — a library designed to enable rapid research on machine learning models. It builds upon a few projects, most notably Lua...},
	urldate = {2019-09-03},
	journal = {NIPS},
	author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
	month = oct,
	year = {2017},
	file = {Full Text PDF:/Users/yl/Zotero/storage/FUGDX32P/Paszke et al. - 2017 - Automatic differentiation in PyTorch.pdf:application/pdf;Snapshot:/Users/yl/Zotero/storage/BA4RQMUW/forum.html:text/html}
}

@article{fey_fast_2019,
	title = {Fast {Graph} {Representation} {Learning} with {PyTorch} {Geometric}},
	url = {http://arxiv.org/abs/1903.02428},
	abstract = {We introduce PyTorch Geometric, a library for deep learning on irregularly structured input data such as graphs, point clouds and manifolds, built upon PyTorch. In addition to general graph data structures and processing methods, it contains a variety of recently published methods from the domains of relational learning and 3D data processing. PyTorch Geometric achieves high data throughput by leveraging sparse GPU acceleration, by providing dedicated CUDA kernels and by introducing efficient mini-batch handling for input examples of different size. In this work, we present the library in detail and perform a comprehensive comparative study of the implemented methods in homogeneous evaluation scenarios.},
	urldate = {2019-09-03},
	journal = {arXiv:1903.02428 [cs, stat]},
	author = {Fey, Matthias and Lenssen, Jan Eric},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.02428},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1903.02428 PDF:/Users/yl/Zotero/storage/MYYTUBPY/Fey and Lenssen - 2019 - Fast Graph Representation Learning with PyTorch Ge.pdf:application/pdf;arXiv.org Snapshot:/Users/yl/Zotero/storage/I8BSGN4C/1903.html:text/html}
}

@inproceedings{gardner_allennlp:_2018,
	address = {Melbourne, Australia},
	title = {{AllenNLP}: {A} {Deep} {Semantic} {Natural} {Language} {Processing} {Platform}},
	shorttitle = {{AllenNLP}},
	url = {https://www.aclweb.org/anthology/W18-2501},
	doi = {10.18653/v1/W18-2501},
	abstract = {Modern natural language processing (NLP) research requires writing code. Ideally this code would provide a precise definition of the approach, easy repeatability of results, and a basis for extending the research. However, many research codebases bury high-level parameters under implementation details, are challenging to run and debug, and are difficult enough to extend that they are more likely to be rewritten. This paper describes AllenNLP, a library for applying deep learning methods to NLP research that addresses these issues with easy-to-use command-line tools, declarative configuration-driven experiments, and modular NLP abstractions. AllenNLP has already increased the rate of research experimentation and the sharing of NLP components at the Allen Institute for Artificial Intelligence, and we are working to have the same impact across the field.},
	urldate = {2019-09-03},
	booktitle = {Proceedings of {Workshop} for {NLP} {Open} {Source} {Software} ({NLP}-{OSS})},
	publisher = {Association for Computational Linguistics},
	author = {Gardner, Matt and Grus, Joel and Neumann, Mark and Tafjord, Oyvind and Dasigi, Pradeep and Liu, Nelson F. and Peters, Matthew and Schmitz, Michael and Zettlemoyer, Luke},
	month = jul,
	year = {2018},
	pages = {1--6},
	file = {Full Text PDF:/Users/yl/Zotero/storage/CQWZ23ID/Gardner et al. - 2018 - AllenNLP A Deep Semantic Natural Language Process.pdf:application/pdf}
}

@article{markidis_nvidia_2018,
	title = {{NVIDIA} {Tensor} {Core} {Programmability}, {Performance} \& {Precision}},
	url = {http://arxiv.org/abs/1803.04014},
	doi = {10.1109/IPDPSW.2018.00091},
	abstract = {The NVIDIA Volta GPU microarchitecture introduces a specialized unit, called "Tensor Core" that performs one matrix-multiply-and-accumulate on 4x4 matrices per clock cycle. The NVIDIA Tesla V100 accelerator, featuring the Volta microarchitecture, provides 640 Tensor Cores with a theoretical peak performance of 125 Tflops/s in mixed precision. In this paper, we investigate current approaches to program NVIDIA Tensor Cores, their performances and the precision loss due to computation in mixed precision. Currently, NVIDIA provides three different ways of programming matrix-multiply-and-accumulate on Tensor Cores: the CUDA Warp Matrix Multiply Accumulate (WMMA) API, CUTLASS, a templated library based on WMMA, and cuBLAS GEMM. After experimenting with different approaches, we found that NVIDIA Tensor Cores can deliver up to 83 Tflops/s in mixed precision on a Tesla V100 GPU, seven and three times the performance in single and half precision respectively. A WMMA implementation of batched GEMM reaches a performance of 4 Tflops/s. While precision loss due to matrix multiplication with half precision input might be critical in many HPC applications, it can be considerably reduced at the cost of increased computation. Our results indicate that HPC applications using matrix multiplications can strongly benefit from using of NVIDIA Tensor Cores.},
	urldate = {2019-09-04},
	journal = {2018 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)},
	author = {Markidis, Stefano and Der Chien, Steven Wei and Laure, Erwin and Peng, Ivy Bo and Vetter, Jeffrey S.},
	month = may,
	year = {2018},
	note = {arXiv: 1803.04014},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Performance},
	pages = {522--531},
	file = {arXiv\:1803.04014 PDF:/Users/yl/Zotero/storage/IDJULHZR/Markidis et al. - 2018 - NVIDIA Tensor Core Programmability, Performance & .pdf:application/pdf;arXiv.org Snapshot:/Users/yl/Zotero/storage/P72KHMT6/1803.html:text/html}
}

@inproceedings{ioffe_batch_2015,
	series = {{ICML}'15},
	title = {Batch {Normalization}: {Accelerating} {Deep} {Network} {Training} by {Reducing} {Internal} {Covariate} {Shift}},
	shorttitle = {Batch {Normalization}},
	url = {http://dl.acm.org/citation.cfm?id=3045118.3045167},
	abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82\% top-5 test error, exceeding the accuracy of human raters.},
	urldate = {2019-09-04},
	booktitle = {Proceedings of the 32Nd {International} {Conference} on {International} {Conference} on {Machine} {Learning} - {Volume} 37},
	publisher = {JMLR.org},
	author = {Ioffe, Sergey and Szegedy, Christian},
	year = {2015},
	note = {event-place: Lille, France},
	pages = {448--456}
}

@article{ba_layer_2016,
	title = {Layer {Normalization}},
	url = {http://arxiv.org/abs/1607.06450},
	abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
	urldate = {2019-09-04},
	journal = {arXiv:1607.06450 [cs, stat]},
	author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
	month = jul,
	year = {2016},
	note = {arXiv: 1607.06450},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1607.06450 PDF:/Users/yl/Zotero/storage/IICTVLMW/Ba et al. - 2016 - Layer Normalization.pdf:application/pdf;arXiv.org Snapshot:/Users/yl/Zotero/storage/IT2EGMV6/1607.html:text/html}
}

@article{ulyanov_instance_2016,
	title = {Instance {Normalization}: {The} {Missing} {Ingredient} for {Fast} {Stylization}},
	shorttitle = {Instance {Normalization}},
	url = {http://arxiv.org/abs/1607.08022},
	abstract = {It this paper we revisit the fast stylization method introduced in Ulyanov et. al. (2016). We show how a small change in the stylization architecture results in a significant qualitative improvement in the generated images. The change is limited to swapping batch normalization with instance normalization, and to apply the latter both at training and testing times. The resulting method can be used to train high-performance architectures for real-time image generation. The code will is made available on github at https://github.com/DmitryUlyanov/texture\_nets. Full paper can be found at arXiv:1701.02096.},
	urldate = {2019-09-04},
	journal = {arXiv:1607.08022 [cs]},
	author = {Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor},
	month = jul,
	year = {2016},
	note = {arXiv: 1607.08022},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1607.08022 PDF:/Users/yl/Zotero/storage/Y4KV59EK/Ulyanov et al. - 2016 - Instance Normalization The Missing Ingredient for.pdf:application/pdf;arXiv.org Snapshot:/Users/yl/Zotero/storage/YESUWVMJ/1607.html:text/html}
}

@inproceedings{devlin_bert:_2019,
	address = {Minneapolis, Minnesota},
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {https://www.aclweb.org/anthology/N19-1423},
	doi = {10.18653/v1/N19-1423},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2019-09-04},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = jun,
	year = {2019},
	pages = {4171--4186},
	file = {Full Text PDF:/Users/yl/Zotero/storage/GB6YGBFZ/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf}
}

@article{yang_xlnet:_2019,
	title = {{XLNet}: {Generalized} {Autoregressive} {Pretraining} for {Language} {Understanding}},
	shorttitle = {{XLNet}},
	url = {http://arxiv.org/abs/1906.08237},
	abstract = {With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, XLNet outperforms BERT on 20 tasks, often by a large margin, and achieves state-of-the-art results on 18 tasks including question answering, natural language inference, sentiment analysis, and document ranking.},
	urldate = {2019-09-04},
	journal = {arXiv:1906.08237 [cs]},
	author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V.},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.08237},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv\:1906.08237 PDF:/Users/yl/Zotero/storage/SJ3U3EMM/Yang et al. - 2019 - XLNet Generalized Autoregressive Pretraining for .pdf:application/pdf;arXiv.org Snapshot:/Users/yl/Zotero/storage/HJWC27U9/1906.html:text/html}
}

@inproceedings{yang_hotpotqa:_2018,
	address = {Brussels, Belgium},
	title = {{HotpotQA}: {A} {Dataset} for {Diverse}, {Explainable} {Multi}-hop {Question} {Answering}},
	shorttitle = {{HotpotQA}},
	url = {https://www.aclweb.org/anthology/D18-1259},
	doi = {10.18653/v1/D18-1259},
	abstract = {Existing question answering (QA) datasets fail to train QA systems to perform complex reasoning and provide explanations for answers. We introduce HotpotQA, a new dataset with 113k Wikipedia-based question-answer pairs with four key features: (1) the questions require finding and reasoning over multiple supporting documents to answer; (2) the questions are diverse and not constrained to any pre-existing knowledge bases or knowledge schemas; (3) we provide sentence-level supporting facts required for reasoning, allowing QA systems to reason with strong supervision and explain the predictions; (4) we offer a new type of factoid comparison questions to test QA systems' ability to extract relevant facts and perform necessary comparison. We show that HotpotQA is challenging for the latest QA systems, and the supporting facts enable models to improve performance and make explainable predictions.},
	urldate = {2019-09-04},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Yang, Zhilin and Qi, Peng and Zhang, Saizheng and Bengio, Yoshua and Cohen, William and Salakhutdinov, Ruslan and Manning, Christopher D.},
	month = oct,
	year = {2018},
	pages = {2369--2380},
	file = {Full Text PDF:/Users/yl/Zotero/storage/M9WDQUYL/Yang et al. - 2018 - HotpotQA A Dataset for Diverse, Explainable Multi.pdf:application/pdf}
}