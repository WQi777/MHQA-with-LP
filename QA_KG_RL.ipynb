{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QA-KG-RL.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yg-li/QA-KG-RL/blob/master/QA_KG_RL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQuAXdFMMZ15",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install --no-cache-dir torch-scatter torch-sparse torch-cluster\n",
        "!pip install torch-geometric\n",
        "! (if [ \"$(pip freeze | grep spacy | cut -d'=' -f 3)\" != \"2.1.3\" ]; then \\\n",
        "     pip uninstall -y spacy; \\\n",
        "     pip install spacy==2.1.3; \\\n",
        "   fi)\n",
        "!pip install neuralcoref\n",
        "!pip install allennlp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZCwquLR9VM1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import json\n",
        "import itertools\n",
        "import re\n",
        "from datetime import datetime\n",
        "\n",
        "import spacy\n",
        "from spacy.matcher import PhraseMatcher\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "import neuralcoref\n",
        "neuralcoref.add_to_pipe(nlp)\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.data import DataLoader\n",
        "from torch_geometric.nn import RGCNConv\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "from allennlp.commands.elmo import ElmoEmbedder\n",
        "elmo = ElmoEmbedder(cuda_device=0 if torch.cuda.is_available() else -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsXK61rj08Lp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "# read in QAngaroo WikiHop\n",
        "wh_data_path = '/gdrive/My Drive/Colab Notebooks/CSML/Project/data/qangaroo_v1.1/wikihop'\n",
        "with open(os.path.join(wh_data_path, 'train.json')) as f:\n",
        "  src = json.loads(f.read())\n",
        "# with open(os.path.join(wh_data_path, 'dev.json')) as f:\n",
        "#   src = json.loads(f.read())\n",
        "\n",
        "# # read in HotpotQA\n",
        "# hpqa_data_path = '/gdrive/My Drive/Colab Notebooks/CSML/Project/data/hotpotqa'\n",
        "# with open(os.path.join(hpqa_data_path, 'hotpot_train_v1.1.json')) as f:\n",
        "#   src = json.loads(f.read())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9qi6k0T9MFM",
        "colab_type": "text"
      },
      "source": [
        "# Encoders and Output Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xQVn9xr9LMm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class QueryEncoder(nn.Module):\n",
        "  def __init__(self, dropout=0):\n",
        "    super(QueryEncoder, self).__init__()\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    self.lstm1 = nn.LSTM(3072, 256, batch_first=True, bidirectional=True)\n",
        "    self.lstm2 = nn.LSTM(512, 128, batch_first=True, bidirectional=True)\n",
        "    self.hidden_map = nn.Linear(256, 128)\n",
        "    self.cell_map = nn.Linear(256, 128)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    # batch_size is always 1 as encoding happens per query\n",
        "    x, (h_n, c_n) = self.lstm1(x)\n",
        "    x = self.dropout(x)\n",
        "    h_n = self.dropout(F.relu(self.hidden_map(h_n)))\n",
        "    c_n = self.dropout(F.relu(self.cell_map(c_n)))\n",
        "    x, (q, c_n) = self.lstm2(x, (h_n, c_n))\n",
        "    q = self.dropout(q.reshape(1, -1, 256))\n",
        "    return q\n",
        "  \n",
        "class CandidateEncoder(nn.Module):\n",
        "  def __init__(self, dropout=0):\n",
        "    super(CandidateEncoder, self).__init__()\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    self.linear1 = nn.Linear(3072, 256)\n",
        "    # the following FF layers applied to concat of query and cand.\n",
        "    self.linear2 = nn.Linear(512, 1024)\n",
        "    self.linear3 = nn.Linear(1024, 512)\n",
        "    \n",
        "  def forward(self, x, q):\n",
        "    x = self.dropout(F.relu(self.linear1(x)))\n",
        "    # TODO: expand q to be the same size of x\n",
        "    x = torch.cat((q, x), dim=-1)\n",
        "    x = self.dropout(F.relu(self.linear2(x)))\n",
        "    x = self.dropout(F.relu(self.linear3(x)))\n",
        "    return x\n",
        "  \n",
        "class OutputLayer(nn.Module):\n",
        "  def __init__(self, dropout=0):\n",
        "    super(OutputLayer, self).__init__()\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    self.linear1 = nn.Linear(768, 256)\n",
        "    self.linear2 = nn.Linear(256, 128)\n",
        "    self.linear3 = nn.Linear(128, 1)\n",
        "    \n",
        "  def forward(self, x, q):\n",
        "    # batch_size is 1 as instances have different number of candidates\n",
        "    # TODO: expand q to be the same size of x\n",
        "    x = torch.cat((q, x), dim=-1)\n",
        "    x = self.dropout(self.linear1(x))\n",
        "    x = self.dropout(self.linear2(x))\n",
        "    x = self.linear3(x)\n",
        "    a = F.log_softmax(x, dim=-1)\n",
        "    return a"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "labw0S-477BQ",
        "colab_type": "text"
      },
      "source": [
        "# Build Graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sd3LnbHdAx4x",
        "colab_type": "text"
      },
      "source": [
        "## Extract nodes and edges & Encode mentions with ELMo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-_ok1U98vxj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "instance = 0\n",
        "query = src[instance].get('query')\n",
        "supports = [text.lower() for text in src[instance].get('supports')]\n",
        "docs = [nlp(text) for text in supports]\n",
        "whole_doc = nlp(' '.join(supports))\n",
        "cands = dict([(v, i) for i, v in \n",
        "              enumerate([cand.lower() for cand in src[instance].get('candidates')])])\n",
        "answer = src[instance].get('answer')\n",
        "\n",
        "query_encoder = QueryEncoder().to(device)\n",
        "cand_encoder = CandidateEncoder().to(device)\n",
        "\n",
        "nodes, node_types, node_embeddings, query_embedding, edges = \\\n",
        "        build_entity_graph(query, docs, whole_doc, cands, answer, query_encoder, cand_encoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UewhVYZsqXsd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_entity_graph(query, docs, whole_doc, cands, answer,\n",
        "                       query_encoder, cand_encoder):\n",
        "  ''' build the entity graph used in Entity-GCN\n",
        "  Args:\n",
        "    query: the query\n",
        "    docs: spacy annotated documents\n",
        "    whole_doc: spacy annotated concatenated documents\n",
        "    cands: candidates\n",
        "    answer: the correct answer\n",
        "    query_encoder: the encoder for query\n",
        "    cand_encoder: the encoder for candidates given query embedding\n",
        "  Returns:\n",
        "    nodes: nodes of the graph \\\\ dict{id : candidate id (-1 if query entity)}\n",
        "    node_types: 1 for answer, 0 for other candidates, -1 for query entity\n",
        "    node_embeddings: the contextualized embedding of mentions of candidates\n",
        "    query_embedding: the emebdding of query\n",
        "    doc_based_edges: edges that connect mentions in the same document \n",
        "    match_edges: edges that connect exact match \\\\ set((node1, node2))\n",
        "    coref_edges: edges that connect mentions in the same coreference chain \n",
        "    compl_edges: edges that connect all nodes that have not been connected by \n",
        "                 any other types of edges \\\\ set((node1, node2))\n",
        "  ''' \n",
        "  # extract the query entity\n",
        "  query_entity = ' '.join(query.split(' ')[1:])\n",
        "  cands[query_entity] = -1\n",
        "  \n",
        "  # matcher for candidates and query entity\n",
        "  matcher = PhraseMatcher(nlp.vocab)\n",
        "  patterns = [nlp.make_doc(cand) for cand in cands]\n",
        "  matcher.add(\"CandList\", None, *patterns)\n",
        "\n",
        "  # get embedding of query, q\n",
        "  query_embedding = query_encoder(torch.tensor(elmo.embed_sentence(query.split(' ')).reshape(1, -1, 3072)).to(device))\n",
        "  # get elmo for all documents\n",
        "  docs_elmo = [torch.tensor(elmo.embed_sentence([w.text for w in doc]).reshape(1, -1, 3072)).to(device) for doc in docs]\n",
        "  print([emb.shape for emb in docs_elmo])\n",
        "  \n",
        "  nodes = {}\n",
        "  node_types = []\n",
        "  node_embeddings = []\n",
        "  with_edges = set()\n",
        "  \n",
        "  # sets to store edges\n",
        "  doc_based_edges = set()\n",
        "  match_edges = set()\n",
        "  coref_edges = set()\n",
        "  compl_edges = set()\n",
        "  \n",
        "  # auxiliary variables for cross-document coreference\n",
        "  out_coref_clusters = [[m.text for m in c.mentions] \n",
        "                        for c in whole_doc._.coref_clusters]\n",
        "  out_coref_tmps = [set()] * len(out_coref_clusters) # nodes in same coref chain\n",
        "  \n",
        "  # accumulate nodes, add the doc_based & coreference edges\n",
        "  for doc_id, doc in enumerate(docs):\n",
        "    matches = matcher(doc)\n",
        "    # text = ' '.join([toc.text for toc in doc])\n",
        "    in_coref_clusters = [[m.text for m in c.mentions] \n",
        "                         for c in doc._.coref_clusters]\n",
        "    in_coref_tmps = [set()] * len(in_coref_clusters) # nodes in same coref chain\n",
        "    doc_tmp = set() # nodes in the same doc\n",
        "    for _, start, end in matches:\n",
        "      match = doc[start:end].text\n",
        "      new_node = len(nodes)\n",
        "      doc_tmp.add(new_node)\n",
        "      nodes[new_node] = cands[match]\n",
        "      node_types.append([1 if match == answer \n",
        "                         else -1 if match == query_entity \n",
        "                         else 0])\n",
        "      match_elmo = docs_elmo[doc_id][:, start:end, :].mean(dim=1, keepdim=True)\n",
        "      node_embeddings.append(cand_encoder(match_elmo, query_embedding))\n",
        "      for i, cluster in enumerate(in_coref_clusters):\n",
        "        if match in cluster:\n",
        "          in_coref_tmps[i].add(new_node)\n",
        "      for i, cluster in enumerate(out_coref_clusters):\n",
        "        if match in cluster:\n",
        "          out_coref_tmps[i].add(new_node)\n",
        "          \n",
        "    for pair in itertools.combinations(doc_tmp, 2):\n",
        "      doc_based_edges.add(pair) # doc_based edges\n",
        "      with_edges.update(pair)\n",
        "    for coref_tmp in in_coref_tmps:\n",
        "      for pair in itertools.combinations(coref_tmp, 2):\n",
        "        coref_edges.add(pair) # within-document coref_edges\n",
        "        with_edges.update(pair)\n",
        "        \n",
        "  # cross-document coref_edges\n",
        "  for coref_tmp in out_coref_tmps:\n",
        "    for pair in itertools.combinations(coref_tmp, 2):\n",
        "      coref_edges.add(pair) # cross-document coref_edges\n",
        "      with_edges.update(pair)\n",
        "      \n",
        "  # add exact match edges\n",
        "  for i, j in itertools.combinations(nodes, 2):\n",
        "    if nodes[i] == nodes[j]:\n",
        "      match_edges.add((i,j))\n",
        "      with_edges.update((i,j))\n",
        "      \n",
        "  # add complement edges\n",
        "  isolated_nodes = set(nodes) - with_edges\n",
        "  if isolated_nodes:\n",
        "    for pair in itertools.combinations(isolated_nodes, 2):\n",
        "      compl_edges.add(pair)\n",
        "      \n",
        "  return nodes, node_types, node_embeddings, query_embedding, \\\n",
        "         [doc_based_edges, match_edges, coref_edges, compl_edges]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VE_E01PXA6ne",
        "colab_type": "text"
      },
      "source": [
        "## Build PyG graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "na1M8d7XqM2C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build graphs from train set of WikiHop\n",
        "query_encoder = QueryEncoder().to(device)\n",
        "cand_encoder = CandidateEncoder().to(device)\n",
        "\n",
        "graphs = []\n",
        "for instance in range(10):#range(len(src)):\n",
        "  print('i:', instance, '|', datetime.now(), flush=True)\n",
        "  query = src[instance].get('query')\n",
        "  supports = [text.lower() for text in src[instance].get('supports')]\n",
        "  docs = [nlp(text) for text in supports]\n",
        "  whole_doc = nlp(' '.join(supports))\n",
        "  cands = dict([(v, i) for i, v in \n",
        "                enumerate([cand.lower() for cand in src[instance].get('candidates')])])\n",
        "  answer = src[instance].get('answer')\n",
        "\n",
        "  \n",
        "\n",
        "  nodes, node_types, node_embeddings, query_embedding, edges = \\\n",
        "    build_entity_graph(query, docs, whole_doc, cands, answer, query_encoder, cand_encoder)\n",
        "  \n",
        "  # build graph in PyG\n",
        "  # nodes\n",
        "  x = torch.tensor(node_embeddings).shape\n",
        "  node_name = list(nodes.values())\n",
        "  tmp = torch.tensor(node_types)\n",
        "  node_mask = (tmp >= 0).to(torch.float) # whether the node is in candidate list\n",
        "  y = (tmp > 0).to(torch.float) # target\n",
        "\n",
        "  # edges\n",
        "  edge_index = torch.zeros(0, 2)\n",
        "  edge_type = torch.zeros(0)\n",
        "  num_relations = 0\n",
        "  for i, e in enumerate(edges):\n",
        "    if len(e) > 0:\n",
        "      num_relations += 1\n",
        "      tmp = torch.tensor(list(e), dtype=torch.float)\n",
        "      # add edges with swapped direction to make the graph undirected\n",
        "      tmp = torch.cat((tmp, torch.index_select(tmp,1,torch.tensor([1,0]))), 0)\n",
        "      edge_index = torch.cat((edge_index, tmp), 0)\n",
        "      edge_type = torch.cat((edge_type, torch.ones(tmp.shape[0]) * i), 0)\n",
        "\n",
        "  data = Data(x=x, node_name=node_name, node_mask=node_mask, \n",
        "              edge_index=edge_index.t().contiguous(), edge_type=edge_type, \n",
        "              num_relations=num_relations, y=y)\n",
        "  graphs.append(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiCynm_0S3L9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# torch.save(graphs, '/gdrive/My Drive/Colab Notebooks/CSML/Project/data/qangaroo_train.pt')\n",
        "# test = torch.load('/gdrive/My Drive/Colab Notebooks/CSML/Project/data/test.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdURez7sCuPz",
        "colab_type": "text"
      },
      "source": [
        "#RGCN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVZwRqqwSIsP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    # three layers are sharing weights\n",
        "    self.conv = RGCNConv(data.num_node_features, 512, \n",
        "                         data.num_relations, num_bases=30)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x489ulHRV71k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train():\n",
        "  # for some epoches\n",
        "    # for each batch\n",
        "      # build graphs using encoders (and ELMO)\n",
        "      # run RGCN on graphs\n",
        "      # output result and backward loss"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}